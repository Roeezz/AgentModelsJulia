{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "using Gen, Statistics, Memoize, Luxor\n",
    "import StatsPlots"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "The previous chapters have shown how to compute optimal actions for agents in MDPs and POMDPs. In many practical applications, this is the goal. For example, when controlling a robot, the goal is for the robot to act optimally given its utility function. When playing the stock market or poker, the goal is make money and one might use an approach based on the POMDP agent model from the [previous chapter](/chapters/3c-pomdp).\n",
    "\n",
    "In other settings, however, the goal is to *learn* or *reason about* an agent based on their behavior. For example, in social science or psychology researchers often seek to learn about people's preferences (denoted $U$) and beliefs (denoted $b$). The relevant *data* (denoted $\\{a_i\\}$) are usually observations of human actions. In this situation, models of optimal action can be used as *generative models* of human actions. The generative model predicts the behavior *given* preferences and beliefs. That is:\n",
    "\n",
    "$\n",
    "P( \\{a_i\\} \\vert U, b)  =: \\text{Generative model of optimal action}\n",
    "$\n",
    "\n",
    "Statistical inference infers the preferences $U$ and beliefs $b$ *given* the observed actions $\\{a_i\\}$. That is:\n",
    "\n",
    "$\n",
    "P( U, b \\vert \\{a_i\\})  =: \\text{Invert generative model via statistical inference}\n",
    "$\n",
    "\n",
    "This approach, using generative models of sequential decision making, has been used to learn preferences and beliefs about education, work, health, and many other topics[^generative].\n",
    "\n",
    "Agent models are also used as generative models in Machine Learning, under the label \"Inverse Reinforcement Learning\" (IRL). One motivation for learning human preferences and beliefs is to give humans helpful recommendations (e.g. for products they are likely to enjoy). A different goal is to build systems that mimic human expert performance. For some tasks, it is hard for humans to directly specify a utility/reward function that is both correct and that can be tractably optimized. An alternative is to *learn* the human's utility function by watching them perform the task. Once learned, the system can use standard RL techniques to optimize the function. This has been applied to building systems to park cars, to fly helicopters, to control human-like bots in videogames, and to play table tennis[^inverse].\n",
    "\n",
    "This chapter provides an array of illustrative examples of learning about agents from their actions. We begin with a concrete example and then provide a general formalization of the inference problem. A virtue of using WebPPL is that doing inference over our existing agent models requires very little extra code.\n",
    "\n",
    "\n",
    "## Learning about an agent from their actions: motivating example\n",
    "\n",
    "Consider the MDP version of Bob's Restaurant Choice problem. Bob is choosing between restaurants, all restaurants are open (and Bob knows this), and Bob also knows the street layout. Previously, we discussed how to compute optimal behavior *given* Bob's utility function over restaurants. Now we infer Bob's utility function *given* observations of the behavior in the codebox:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: restaurant constants, donutSouthTrajectory\n",
    "var ___ = ' ';\n",
    "var DN = { name: 'Donut N' };\n",
    "var DS = { name: 'Donut S' };\n",
    "var V = { name: 'Veg' };\n",
    "var N = { name: 'Noodle' };\n",
    "\n",
    "var donutSouthTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"l\"],\n",
    "  [{\"loc\":[2,1],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"l\"],\n",
    "  [{\"loc\":[1,1],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[2,1]},\"l\"],\n",
    "  [{\"loc\":[0,1],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[1,1]},\"d\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[0,1],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":true,\"timeLeft\":7,\"previousLoc\":[0,0],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "///\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var world = makeGridWorldMDP({ grid, start: [3, 1] }).world;\n",
    "\n",
    "viz.gridworld(world, { trajectory: donutSouthTrajectory });\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "actions = [:up,:down,:left,:right]\n",
    "grid_world = [\n",
    "     \"#\"   \"#\"   \"#\"   \"#\"   \"V\"   \"#\" ; #1\n",
    "     \"#\"   \"#\"   \"#\"  \"___\" \"___\" \"___\"; #2\n",
    "     \"#\"   \"#\"  \"DN\"  \"___\"  \"#\"  \"___\"; #3\n",
    "     \"#\"   \"#\"   \"#\"  \"___\"  \"#\"  \"___\"; #4\n",
    "     \"#\"   \"#\"   \"#\"  \"___\" \"___\" \"___\"; #5\n",
    "     \"#\"   \"#\"   \"#\"  \"___\"  \"#\"   \"N\" ; #6\n",
    "    \"___\" \"___\" \"___\" \"___\"  \"#\"   \"#\" ; #7\n",
    "    \"DS\"   \"#\"   \"#\"  \"___\"  \"#\"   \"#\" ; #8\n",
    "]#    1     2     3     4     5     6\n",
    "     \n",
    "\n",
    "grid_world_utilities = Dict(\n",
    "  \"DS\" => 1, \n",
    "  \"DN\" => 1, \n",
    "  \"V\" => 3,\n",
    "  \"N\" => 2, \n",
    "  \"___\"=> -0.1\n",
    ")\n",
    "n_rows, n_cols = size(grid_world)\n",
    "grid_size = (n_rows, n_cols)\n",
    "\n",
    "grid_world_colors = Dict(\n",
    "    \"DS\" => \"pink\", \n",
    "    \"DN\" => \"pink\",\n",
    "    \"V\" => \"green\",\n",
    "    \"N\" => \"yellow\", \n",
    "    \"___\"=> \"gray\",\n",
    "    \"#\" => \"black\"\n",
    ")\n",
    "\n",
    "grid_world_names = Dict(\n",
    "    \"DS\" => \"DONUT SOUTH\",\n",
    "    \"DN\" => \"DONUT NORTH\",\n",
    "    \"V\" => \"VEGI\",\n",
    "    \"N\" => \"NOODLES\", \n",
    "    \"___\"=> \"\",\n",
    "    \"#\" => \"\"\n",
    ")\n",
    "\n",
    "function get_state(n)\n",
    "    row = (n-1) ÷ (grid_size[2] + 1)\n",
    "    col = (n-1) % (grid_size[2] + 1)\n",
    "    return [row, col]\n",
    "end\n",
    "\n",
    "function get_tile(state)\n",
    "    return grid_world[state...]\n",
    "end\n",
    "\n",
    "target_traj = [[7,4], [7,3], [7,2], [7,1], [8,1]]\n",
    "\n",
    "demo = Movie((n_cols + 2) * 100, (n_rows + 2) * 100, \"agent trajectory\", 1:length(target_traj))\n",
    "function backdrop(scene, framenumber)\n",
    "    background(\"white\")\n",
    "end\n",
    "\n",
    "function frame(scene, framenumber)\n",
    "    tiles = Tiler((n_cols + 1) * 100, (n_rows + 1) * 100, n_rows + 1,n_cols + 1, margin=0)\n",
    "    agent_state = target_traj[framenumber]\n",
    "    for (pos, n) in tiles\n",
    "        state = get_state(n)\n",
    "        if state[1] == 0 || state[2] == 0\n",
    "            if state != [0,0]\n",
    "                fontsize(18)\n",
    "                fontface(\"\")\n",
    "                sethue(\"black\")\n",
    "                textcentered(string(max(state...)), pos + Point(0, 5))\n",
    "            end\n",
    "            continue\n",
    "        end\n",
    "        tile = get_tile(state)\n",
    "        sethue(grid_world_colors[tile])\n",
    "        box(pos, tiles.tilewidth, tiles.tileheight, :fill)\n",
    "        fontsize(13)\n",
    "        fontface(\"Cambria Bold\")\n",
    "        sethue(\"black\")\n",
    "        textcentered(grid_world_names[tile], pos + Point(0, 5))\n",
    "        if (state == agent_state)\n",
    "            sethue(\"blue\")\n",
    "            circle(pos, min(tiles.tilewidth, tiles.tileheight) / 3, :fill)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "animate(demo, [\n",
    "    Scene(demo, backdrop),\n",
    "    Scene(demo, frame)\n",
    "    ],\n",
    "    framerate=5,\n",
    "    creategif=true)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Frames for animation \"agent trajectory\" are being stored in directory: \n",
      "│ \t C:\\Users\\Roie\\AppData\\Local\\Temp\\jl_ZD6Jki\n",
      "└ @ Luxor C:\\Users\\Roie\\.julia\\packages\\Luxor\\XHa4Y\\src\\animate.jl:178\n",
      "┌ Info: ... 5 frames saved in directory:\n",
      "│ \t C:\\Users\\Roie\\AppData\\Local\\Temp\\jl_ZD6Jki\n",
      "└ @ Luxor C:\\Users\\Roie\\.julia\\packages\\Luxor\\XHa4Y\\src\\animate.jl:200\n",
      "┌ Info: GIF is: C:\\Users\\Roie\\AppData\\Local\\Temp\\jl_ZD6Jki/agent trajectory.gif\n",
      "└ @ Luxor C:\\Users\\Roie\\.julia\\packages\\Luxor\\XHa4Y\\src\\animate.jl:229\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Luxor.AnimatedGif(\"C:\\\\Users\\\\Roie\\\\AppData\\\\Local\\\\Temp\\\\jl_ZD6Jki/agent trajectory.gif\")"
      ],
      "text/html": [
       "<img src=\"data:image/gif;base64,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\" />"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From Bob's actions, we infer that he probably prefers the Donut Store to the other restaurants. An alternative explanation is that Bob cares most about saving time. He might prefer Veg (the Vegetarian Cafe) but his preference is not strong enough to spend extra time getting there.\n",
    "\n",
    "In this first example of inference, Bob's preference for saving time is held fixed and we infer (given the actions shown above) Bob's preference for the different restaurants. We model Bob using the MDP agent model from [Chapter 3.1](/chapters/3a-mdp.html). We place a uniform prior over three possible utility functions for Bob: one favoring Donut, one favoring Veg and one favoring Noodle. We compute a Bayesian posterior over these utility functions *given* Bob's observed behavior. Since the world is practically deterministic (with softmax parameter $\\alpha$ set high), we just compare Bob's predicted states under each utility function to the states actually observed. To predict Bob's states for each utility function, we use the function `simulate` from [Chapter 3.1](/chapters/3a-mdp.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: restaurant constants, donutSouthTrajectory\n",
    "var ___ = ' ';\n",
    "var DN = { name: 'Donut N' };\n",
    "var DS = { name: 'Donut S' };\n",
    "var V = { name: 'Veg' };\n",
    "var N = { name: 'Noodle' };\n",
    "\n",
    "var donutSouthTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"l\"],\n",
    "  [{\"loc\":[2,1],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"l\"],\n",
    "  [{\"loc\":[1,1],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[2,1]},\"l\"],\n",
    "  [{\"loc\":[0,1],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[1,1]},\"d\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[0,1],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":true,\"timeLeft\":7,\"previousLoc\":[0,0],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "///\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  noReverse: true,\n",
    "  maxTimeAtRestaurant: 2,\n",
    "  start: [3, 1],\n",
    "  totalTime: 11\n",
    "});\n",
    "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
    "var world = mdp.world;\n",
    "\n",
    "var startState = donutSouthTrajectory[0][0];\n",
    "\n",
    "var utilityTablePrior = function() {\n",
    "  var baseUtilityTable = {\n",
    "    'Donut S': 1,\n",
    "    'Donut N': 1,\n",
    "    'Veg': 1,\n",
    "    'Noodle': 1,\n",
    "    'timeCost': -0.04\n",
    "  };\n",
    "  return uniformDraw(\n",
    "    [{ table: extend(baseUtilityTable, { 'Donut N': 2, 'Donut S': 2 }),\n",
    "       favourite: 'donut' },\n",
    "     { table: extend(baseUtilityTable, { Veg: 2 }),\n",
    "       favourite: 'veg' },\n",
    "     { table: extend(baseUtilityTable, { Noodle: 2 }),\n",
    "       favourite: 'noodle' }]\n",
    "  );\n",
    "};\n",
    "\n",
    "var posterior = Infer({ model() {\n",
    "  var utilityTableAndFavourite = utilityTablePrior();\n",
    "  var utilityTable = utilityTableAndFavourite.table;\n",
    "  var favourite = utilityTableAndFavourite.favourite;\n",
    "\n",
    "  var utility = makeUtilityFunction(utilityTable);\n",
    "  var params = {\n",
    "    utility,\n",
    "    alpha: 2\n",
    "  };\n",
    "  var agent = makeMDPAgent(params, world);\n",
    "\n",
    "  var predictedStateAction = simulateMDP(startState, world, agent, 'stateAction');\n",
    "  condition(_.isEqual(donutSouthTrajectory, predictedStateAction));\n",
    "  return { favourite };\n",
    "}});\n",
    "\n",
    "viz(posterior);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "CONTROL_FACTOR = 50\n",
    "AMOUNT_AGENT = 1000\n",
    "\n",
    "function transition(state,action)\n",
    "    if action == :down\n",
    "        return [state[1]+1,state[2]]\n",
    "    elseif action == :up\n",
    "        return [state[1]-1, state[2]]\n",
    "    elseif action == :left\n",
    "        return [state[1], state[2]-1]\n",
    "    elseif action == :right\n",
    "        return [state[1], state[2]+1]\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_tile(state)\n",
    "    return grid_world[state...]\n",
    "end\n",
    "\n",
    "function state_to_actions(state)\n",
    "    actions = []\n",
    "    if state[1] < grid_size[1] && get_tile(transition(state,:down)) != \"#\" \n",
    "        push!(actions,:down)\n",
    "    end\n",
    "    if state[1] > 1 && get_tile(transition(state,:up)) != \"#\"\n",
    "        push!(actions,:up)\n",
    "    end \n",
    "    if state[2] > 1 && get_tile(transition(state,:left)) != \"#\"\n",
    "        push!(actions, :left)\n",
    "    end \n",
    "    if state[2] < grid_size[2] && get_tile(transition(state,:right)) != \"#\"\n",
    "        push!(actions,:right)\n",
    "    end\n",
    "    return actions\n",
    "end\n",
    "\n",
    "function is_terminal_state(state)\n",
    "    tile = get_tile(state)\n",
    "    return tile == \"V\" || tile == \"DN\" || tile == \"DS\" || tile == \"N\"\n",
    "end\n",
    "\n",
    "function make_agent(utility_table)\n",
    "    function utility(state)\n",
    "        return utility_table[get_tile(state)]\n",
    "    end\n",
    "    \n",
    "\n",
    "    @gen function act(state, time_left)\n",
    "        possible_actions = state_to_actions(state)\n",
    "        action_index = @trace(uniform_discrete(1,length(possible_actions)),:action_index)\n",
    "        next_action = possible_actions[action_index]\n",
    "        eu = expected_utility(state, next_action, time_left)\n",
    "        @trace(bernoulli(exp(CONTROL_FACTOR * eu)), :factor)\n",
    "        return action_index\n",
    "    end\n",
    "\n",
    "    @memoize Dict function run_act(state, time_left)\n",
    "        action_indices = []\n",
    "        trace, = generate(act, (state, time_left), choicemap((:factor,1)))\n",
    "        for i = 1:AMOUNT_AGENT\n",
    "            trace, = Gen.mh(trace, select(:action_index))\n",
    "            push!(action_indices, get_retval(trace))\n",
    "        end\n",
    "        return action_indices\n",
    "    end\n",
    "\n",
    "    @gen function reward(state, action, time_left)\n",
    "        next_state = transition(state, action)\n",
    "        action_indices = run_act(next_state, time_left)\n",
    "        rand_choice = @trace(uniform_discrete(1, length(action_indices)), :rand_choice)\n",
    "        possible_actions = state_to_actions(next_state)\n",
    "        next_action_idx = action_indices[rand_choice]\n",
    "        next_action = possible_actions[next_action_idx]\n",
    "        return expected_utility(next_state, next_action, time_left)\n",
    "    end\n",
    "\n",
    "    @memoize Dict function run_reward(state, action, time_left)\n",
    "        rewards = []\n",
    "        trace, = generate(reward, (state, action, time_left))\n",
    "        for i =1:AMOUNT_AGENT\n",
    "            trace, = Gen.mh(trace, select(:rand_choice))\n",
    "            push!(rewards, get_retval(trace))\n",
    "        end\n",
    "        return rewards\n",
    "    end\n",
    "\n",
    "    @memoize Dict function expected_utility(state, action, time_left)\n",
    "        u = utility(state)\n",
    "        new_time_left = time_left - 1\n",
    "        \n",
    "        if is_terminal_state(state) || new_time_left == 0\n",
    "            return u\n",
    "        else\n",
    "            return u + mean(run_reward(state, action, new_time_left))\n",
    "        end\n",
    "    end\n",
    "    return run_act\n",
    "end\n",
    "\n",
    "@memoize Dict function simulate_agent(utility_table, start_state, total_time)\n",
    "    states = []\n",
    "    run_act = make_agent(utility_table)\n",
    "    next_state = start_state\n",
    "    while !is_terminal_state(next_state) && total_time > 0\n",
    "        push!(states, next_state)\n",
    "        action_indices = run_act(next_state, total_time)\n",
    "        possible_actions =  state_to_actions(next_state)\n",
    "        rand_choice = uniform_discrete(1, length(action_indices))\n",
    "        next_action_idx = action_indices[rand_choice]\n",
    "        next_action = possible_actions[next_action_idx]\n",
    "        next_state = transition(next_state, next_action)\n",
    "        total_time -= 1\n",
    "    end\n",
    "    push!(states, next_state)\n",
    "    return states\n",
    "end;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "AMOUNT_POSTERIOR = 1000\n",
    "\n",
    "actions = [:up,:down,:left,:right]\n",
    "grid_world = [\n",
    "     \"#\"   \"#\"   \"#\"   \"#\"   \"V\"   \"#\" ; #1\n",
    "     \"#\"   \"#\"   \"#\"  \"___\" \"___\" \"___\"; #2\n",
    "     \"#\"   \"#\"  \"DN\"  \"___\"  \"#\"  \"___\"; #3\n",
    "     \"#\"   \"#\"   \"#\"  \"___\"  \"#\"  \"___\"; #4\n",
    "     \"#\"   \"#\"   \"#\"  \"___\" \"___\" \"___\"; #5\n",
    "     \"#\"   \"#\"   \"#\"  \"___\"  \"#\"   \"N\" ; #6\n",
    "    \"___\" \"___\" \"___\" \"___\"  \"#\"   \"#\" ; #7\n",
    "    \"DS\"   \"#\"   \"#\"  \"___\"  \"#\"   \"#\" ; #8\n",
    "]#    1     2     3     4     5     6\n",
    "n_rows, n_cols = size(grid_world)\n",
    "grid_size = (n_rows, n_cols)\n",
    "\n",
    "start_state = target_traj[1]\n",
    "\n",
    "options = [\"donut\", \"veg\", \"noodle\"]\n",
    "\n",
    "function utility_table_prior(option_idx)\n",
    "    base_table = Dict(\n",
    "        \"DS\" => 1, \n",
    "        \"DN\" => 1, \n",
    "        \"V\" => 1,\n",
    "        \"N\" => 1, \n",
    "        \"___\"=> -0.04\n",
    "    )\n",
    "    option = options[option_idx]\n",
    "    ret_table = Dict()\n",
    "    if option == \"donut\"\n",
    "        base_table[\"DS\"] = 2\n",
    "        base_table[\"DN\"] = 2\n",
    "    elseif option == \"veg\"\n",
    "        base_table[\"V\"] = 2\n",
    "    elseif option == \"noodle\"\n",
    "        base_table[\"N\"] = 2\n",
    "    end\n",
    "    ret_table[\"table\"] = base_table\n",
    "    ret_table[\"favorite\"] = option\n",
    "    return ret_table\n",
    "end\n",
    "counters = [0, 0, 0]\n",
    "@gen function posterior(start_state)\n",
    "    option_idx = @trace(uniform_discrete(1, length(options)), :option_idx)\n",
    "    utility_table_and_favorite = utility_table_prior(option_idx)\n",
    "    utility_table = utility_table_and_favorite[\"table\"]\n",
    "    favorite = utility_table_and_favorite[\"favorite\"]\n",
    "    sim_traj = simulate_agent(utility_table, start_state, 15)\n",
    "    # condition\n",
    "    if sim_traj == target_traj\n",
    "        @trace(bernoulli(0.99), :equal_traj)\n",
    "    else\n",
    "        @trace(bernoulli(0.01), :equal_traj)\n",
    "    end\n",
    "    return favorite\n",
    "end\n",
    "\n",
    "function run_posterior(start_state)\n",
    "    favorites = []\n",
    "    trace, = generate(posterior, (start_state,), choicemap((:equal_traj,true)))\n",
    "    for i = 1:AMOUNT_POSTERIOR\n",
    "        trace, = Gen.mh(trace, select(:option_idx))\n",
    "        push!(favorites, get_retval(trace))\n",
    "    end\n",
    "    return favorites\n",
    "end;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "favorites = run_posterior(start_state)\n",
    "# print(favorites)\n",
    "probabilties = Dict([(option,count(favorite-> favorite==option, favorites) / AMOUNT_POSTERIOR) for option in options])\n",
    "StatsPlots.bar(probabilties, label=\"picked favorite\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip840\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip840)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip841\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip840)\" d=\"\nM186.274 1486.45 L2352.76 1486.45 L2352.76 47.2441 L186.274 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip842\">\n    <rect x=\"186\" y=\"47\" width=\"2167\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  580.886,1486.45 580.886,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1269.51,1486.45 1269.51,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1958.14,1486.45 1958.14,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  580.886,1486.45 580.886,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1269.51,1486.45 1269.51,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1958.14,1486.45 1958.14,1469.18 \n  \"/>\n<path clip-path=\"url(#clip840)\" d=\"M525.771 1531.47 L525.771 1547.12 L521.511 1547.12 L521.511 1531.61 Q521.511 1527.93 520.076 1526.1 Q518.641 1524.27 515.771 1524.27 Q512.322 1524.27 510.331 1526.47 Q508.34 1528.67 508.34 1532.47 L508.34 1547.12 L504.058 1547.12 L504.058 1521.19 L508.34 1521.19 L508.34 1525.22 Q509.868 1522.88 511.928 1521.73 Q514.011 1520.57 516.72 1520.57 Q521.187 1520.57 523.479 1523.35 Q525.771 1526.1 525.771 1531.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M544.312 1524.18 Q540.886 1524.18 538.895 1526.87 Q536.905 1529.53 536.905 1534.18 Q536.905 1538.83 538.872 1541.52 Q540.863 1544.18 544.312 1544.18 Q547.715 1544.18 549.706 1541.49 Q551.696 1538.81 551.696 1534.18 Q551.696 1529.57 549.706 1526.89 Q547.715 1524.18 544.312 1524.18 M544.312 1520.57 Q549.868 1520.57 553.039 1524.18 Q556.21 1527.79 556.21 1534.18 Q556.21 1540.55 553.039 1544.18 Q549.868 1547.79 544.312 1547.79 Q538.733 1547.79 535.562 1544.18 Q532.414 1540.55 532.414 1534.18 Q532.414 1527.79 535.562 1524.18 Q538.733 1520.57 544.312 1520.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M573.317 1524.18 Q569.891 1524.18 567.9 1526.87 Q565.909 1529.53 565.909 1534.18 Q565.909 1538.83 567.877 1541.52 Q569.868 1544.18 573.317 1544.18 Q576.719 1544.18 578.71 1541.49 Q580.701 1538.81 580.701 1534.18 Q580.701 1529.57 578.71 1526.89 Q576.719 1524.18 573.317 1524.18 M573.317 1520.57 Q578.872 1520.57 582.043 1524.18 Q585.215 1527.79 585.215 1534.18 Q585.215 1540.55 582.043 1544.18 Q578.872 1547.79 573.317 1547.79 Q567.738 1547.79 564.567 1544.18 Q561.418 1540.55 561.418 1534.18 Q561.418 1527.79 564.567 1524.18 Q567.738 1520.57 573.317 1520.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M609.335 1525.13 L609.335 1511.1 L613.594 1511.1 L613.594 1547.12 L609.335 1547.12 L609.335 1543.23 Q607.992 1545.55 605.932 1546.68 Q603.895 1547.79 601.025 1547.79 Q596.326 1547.79 593.363 1544.04 Q590.423 1540.29 590.423 1534.18 Q590.423 1528.07 593.363 1524.32 Q596.326 1520.57 601.025 1520.57 Q603.895 1520.57 605.932 1521.7 Q607.992 1522.81 609.335 1525.13 M594.821 1534.18 Q594.821 1538.88 596.742 1541.56 Q598.687 1544.23 602.066 1544.23 Q605.446 1544.23 607.39 1541.56 Q609.335 1538.88 609.335 1534.18 Q609.335 1529.48 607.39 1526.82 Q605.446 1524.13 602.066 1524.13 Q598.687 1524.13 596.742 1526.82 Q594.821 1529.48 594.821 1534.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M622.367 1511.1 L626.626 1511.1 L626.626 1547.12 L622.367 1547.12 L622.367 1511.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M657.714 1533.09 L657.714 1535.18 L638.131 1535.18 Q638.409 1539.57 640.77 1541.89 Q643.154 1544.18 647.39 1544.18 Q649.844 1544.18 652.136 1543.58 Q654.45 1542.98 656.719 1541.77 L656.719 1545.8 Q654.427 1546.77 652.02 1547.28 Q649.612 1547.79 647.136 1547.79 Q640.932 1547.79 637.298 1544.18 Q633.687 1540.57 633.687 1534.41 Q633.687 1528.05 637.113 1524.32 Q640.562 1520.57 646.395 1520.57 Q651.626 1520.57 654.659 1523.95 Q657.714 1527.31 657.714 1533.09 M653.455 1531.84 Q653.409 1528.35 651.487 1526.26 Q649.589 1524.18 646.441 1524.18 Q642.876 1524.18 640.724 1526.19 Q638.594 1528.21 638.27 1531.87 L653.455 1531.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1221.41 1525.13 L1221.41 1511.1 L1225.67 1511.1 L1225.67 1547.12 L1221.41 1547.12 L1221.41 1543.23 Q1220.07 1545.55 1218.01 1546.68 Q1215.97 1547.79 1213.1 1547.79 Q1208.4 1547.79 1205.44 1544.04 Q1202.5 1540.29 1202.5 1534.18 Q1202.5 1528.07 1205.44 1524.32 Q1208.4 1520.57 1213.1 1520.57 Q1215.97 1520.57 1218.01 1521.7 Q1220.07 1522.81 1221.41 1525.13 M1206.9 1534.18 Q1206.9 1538.88 1208.82 1541.56 Q1210.77 1544.23 1214.14 1544.23 Q1217.52 1544.23 1219.47 1541.56 Q1221.41 1538.88 1221.41 1534.18 Q1221.41 1529.48 1219.47 1526.82 Q1217.52 1524.13 1214.14 1524.13 Q1210.77 1524.13 1208.82 1526.82 Q1206.9 1529.48 1206.9 1534.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1244.49 1524.18 Q1241.07 1524.18 1239.08 1526.87 Q1237.08 1529.53 1237.08 1534.18 Q1237.08 1538.83 1239.05 1541.52 Q1241.04 1544.18 1244.49 1544.18 Q1247.89 1544.18 1249.89 1541.49 Q1251.88 1538.81 1251.88 1534.18 Q1251.88 1529.57 1249.89 1526.89 Q1247.89 1524.18 1244.49 1524.18 M1244.49 1520.57 Q1250.05 1520.57 1253.22 1524.18 Q1256.39 1527.79 1256.39 1534.18 Q1256.39 1540.55 1253.22 1544.18 Q1250.05 1547.79 1244.49 1547.79 Q1238.91 1547.79 1235.74 1544.18 Q1232.59 1540.55 1232.59 1534.18 Q1232.59 1527.79 1235.74 1524.18 Q1238.91 1520.57 1244.49 1520.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1285 1531.47 L1285 1547.12 L1280.74 1547.12 L1280.74 1531.61 Q1280.74 1527.93 1279.31 1526.1 Q1277.87 1524.27 1275 1524.27 Q1271.55 1524.27 1269.56 1526.47 Q1267.57 1528.67 1267.57 1532.47 L1267.57 1547.12 L1263.29 1547.12 L1263.29 1521.19 L1267.57 1521.19 L1267.57 1525.22 Q1269.1 1522.88 1271.16 1521.73 Q1273.24 1520.57 1275.95 1520.57 Q1280.42 1520.57 1282.71 1523.35 Q1285 1526.1 1285 1531.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1293.06 1536.89 L1293.06 1521.19 L1297.32 1521.19 L1297.32 1536.73 Q1297.32 1540.41 1298.75 1542.26 Q1300.19 1544.09 1303.06 1544.09 Q1306.51 1544.09 1308.5 1541.89 Q1310.51 1539.69 1310.51 1535.89 L1310.51 1521.19 L1314.77 1521.19 L1314.77 1547.12 L1310.51 1547.12 L1310.51 1543.14 Q1308.96 1545.5 1306.9 1546.66 Q1304.86 1547.79 1302.15 1547.79 Q1297.69 1547.79 1295.37 1545.01 Q1293.06 1542.24 1293.06 1536.89 M1303.77 1520.57 L1303.77 1520.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1327.76 1513.83 L1327.76 1521.19 L1336.53 1521.19 L1336.53 1524.5 L1327.76 1524.5 L1327.76 1538.58 Q1327.76 1541.75 1328.61 1542.65 Q1329.49 1543.56 1332.15 1543.56 L1336.53 1543.56 L1336.53 1547.12 L1332.15 1547.12 Q1327.22 1547.12 1325.35 1545.29 Q1323.47 1543.44 1323.47 1538.58 L1323.47 1524.5 L1320.35 1524.5 L1320.35 1521.19 L1323.47 1521.19 L1323.47 1513.83 L1327.76 1513.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1917.35 1521.19 L1921.86 1521.19 L1929.96 1542.95 L1938.06 1521.19 L1942.58 1521.19 L1932.85 1547.12 L1927.07 1547.12 L1917.35 1521.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1970.63 1533.09 L1970.63 1535.18 L1951.05 1535.18 Q1951.33 1539.57 1953.69 1541.89 Q1956.07 1544.18 1960.31 1544.18 Q1962.76 1544.18 1965.05 1543.58 Q1967.37 1542.98 1969.64 1541.77 L1969.64 1545.8 Q1967.35 1546.77 1964.94 1547.28 Q1962.53 1547.79 1960.05 1547.79 Q1953.85 1547.79 1950.22 1544.18 Q1946.6 1540.57 1946.6 1534.41 Q1946.6 1528.05 1950.03 1524.32 Q1953.48 1520.57 1959.31 1520.57 Q1964.54 1520.57 1967.58 1523.95 Q1970.63 1527.31 1970.63 1533.09 M1966.37 1531.84 Q1966.33 1528.35 1964.41 1526.26 Q1962.51 1524.18 1959.36 1524.18 Q1955.79 1524.18 1953.64 1526.19 Q1951.51 1528.21 1951.19 1531.87 L1966.37 1531.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1994.68 1533.86 Q1994.68 1529.23 1992.76 1526.68 Q1990.86 1524.13 1987.41 1524.13 Q1983.99 1524.13 1982.07 1526.68 Q1980.17 1529.23 1980.17 1533.86 Q1980.17 1538.46 1982.07 1541.01 Q1983.99 1543.56 1987.41 1543.56 Q1990.86 1543.56 1992.76 1541.01 Q1994.68 1538.46 1994.68 1533.86 M1998.94 1543.9 Q1998.94 1550.52 1996 1553.74 Q1993.06 1556.98 1987 1556.98 Q1984.75 1556.98 1982.76 1556.63 Q1980.77 1556.31 1978.9 1555.62 L1978.9 1551.47 Q1980.77 1552.49 1982.6 1552.98 Q1984.43 1553.46 1986.33 1553.46 Q1990.52 1553.46 1992.6 1551.26 Q1994.68 1549.09 1994.68 1544.67 L1994.68 1542.56 Q1993.36 1544.85 1991.3 1545.99 Q1989.24 1547.12 1986.37 1547.12 Q1981.6 1547.12 1978.69 1543.49 Q1975.77 1539.85 1975.77 1533.86 Q1975.77 1527.84 1978.69 1524.2 Q1981.6 1520.57 1986.37 1520.57 Q1989.24 1520.57 1991.3 1521.7 Q1993.36 1522.84 1994.68 1525.13 L1994.68 1521.19 L1998.94 1521.19 L1998.94 1543.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  186.274,1445.72 2352.76,1445.72 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  186.274,1101.81 2352.76,1101.81 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  186.274,757.904 2352.76,757.904 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  186.274,413.999 2352.76,413.999 \n  \"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  186.274,70.0932 2352.76,70.0932 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,1486.45 186.274,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,1445.72 212.272,1445.72 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,1101.81 212.272,1101.81 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,757.904 212.272,757.904 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,413.999 212.272,413.999 \n  \"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  186.274,70.0932 212.272,70.0932 \n  \"/>\n<path clip-path=\"url(#clip840)\" d=\"M62.9365 1431.51 Q59.3254 1431.51 57.4967 1435.08 Q55.6912 1438.62 55.6912 1445.75 Q55.6912 1452.86 57.4967 1456.42 Q59.3254 1459.96 62.9365 1459.96 Q66.5707 1459.96 68.3763 1456.42 Q70.205 1452.86 70.205 1445.75 Q70.205 1438.62 68.3763 1435.08 Q66.5707 1431.51 62.9365 1431.51 M62.9365 1427.81 Q68.7467 1427.81 71.8022 1432.42 Q74.8809 1437 74.8809 1445.75 Q74.8809 1454.48 71.8022 1459.08 Q68.7467 1463.67 62.9365 1463.67 Q57.1264 1463.67 54.0477 1459.08 Q50.9921 1454.48 50.9921 1445.75 Q50.9921 1437 54.0477 1432.42 Q57.1264 1427.81 62.9365 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M83.0984 1457.12 L87.9827 1457.12 L87.9827 1463 L83.0984 1463 L83.0984 1457.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M108.168 1431.51 Q104.557 1431.51 102.728 1435.08 Q100.922 1438.62 100.922 1445.75 Q100.922 1452.86 102.728 1456.42 Q104.557 1459.96 108.168 1459.96 Q111.802 1459.96 113.608 1456.42 Q115.436 1452.86 115.436 1445.75 Q115.436 1438.62 113.608 1435.08 Q111.802 1431.51 108.168 1431.51 M108.168 1427.81 Q113.978 1427.81 117.033 1432.42 Q120.112 1437 120.112 1445.75 Q120.112 1454.48 117.033 1459.08 Q113.978 1463.67 108.168 1463.67 Q102.358 1463.67 99.2789 1459.08 Q96.2234 1454.48 96.2234 1445.75 Q96.2234 1437 99.2789 1432.42 Q102.358 1427.81 108.168 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M138.33 1431.51 Q134.719 1431.51 132.89 1435.08 Q131.084 1438.62 131.084 1445.75 Q131.084 1452.86 132.89 1456.42 Q134.719 1459.96 138.33 1459.96 Q141.964 1459.96 143.769 1456.42 Q145.598 1452.86 145.598 1445.75 Q145.598 1438.62 143.769 1435.08 Q141.964 1431.51 138.33 1431.51 M138.33 1427.81 Q144.14 1427.81 147.195 1432.42 Q150.274 1437 150.274 1445.75 Q150.274 1454.48 147.195 1459.08 Q144.14 1463.67 138.33 1463.67 Q132.519 1463.67 129.441 1459.08 Q126.385 1454.48 126.385 1445.75 Q126.385 1437 129.441 1432.42 Q132.519 1427.81 138.33 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M63.9319 1087.61 Q60.3208 1087.61 58.4921 1091.17 Q56.6865 1094.72 56.6865 1101.84 Q56.6865 1108.95 58.4921 1112.52 Q60.3208 1116.06 63.9319 1116.06 Q67.5661 1116.06 69.3717 1112.52 Q71.2004 1108.95 71.2004 1101.84 Q71.2004 1094.72 69.3717 1091.17 Q67.5661 1087.61 63.9319 1087.61 M63.9319 1083.91 Q69.742 1083.91 72.7976 1088.51 Q75.8763 1093.09 75.8763 1101.84 Q75.8763 1110.57 72.7976 1115.18 Q69.742 1119.76 63.9319 1119.76 Q58.1217 1119.76 55.043 1115.18 Q51.9875 1110.57 51.9875 1101.84 Q51.9875 1093.09 55.043 1088.51 Q58.1217 1083.91 63.9319 1083.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M84.0938 1113.21 L88.978 1113.21 L88.978 1119.09 L84.0938 1119.09 L84.0938 1113.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M103.191 1115.15 L119.51 1115.15 L119.51 1119.09 L97.566 1119.09 L97.566 1115.15 Q100.228 1112.4 104.811 1107.77 Q109.418 1103.12 110.598 1101.78 Q112.844 1099.25 113.723 1097.52 Q114.626 1095.76 114.626 1094.07 Q114.626 1091.31 112.682 1089.58 Q110.76 1087.84 107.658 1087.84 Q105.459 1087.84 103.006 1088.6 Q100.575 1089.37 97.7974 1090.92 L97.7974 1086.2 Q100.621 1085.06 103.075 1084.48 Q105.529 1083.91 107.566 1083.91 Q112.936 1083.91 116.131 1086.59 Q119.325 1089.28 119.325 1093.77 Q119.325 1095.9 118.515 1097.82 Q117.728 1099.72 115.621 1102.31 Q115.043 1102.98 111.941 1106.2 Q108.839 1109.39 103.191 1115.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M129.371 1084.53 L147.728 1084.53 L147.728 1088.47 L133.654 1088.47 L133.654 1096.94 Q134.672 1096.59 135.691 1096.43 Q136.709 1096.24 137.728 1096.24 Q143.515 1096.24 146.894 1099.41 Q150.274 1102.59 150.274 1108 Q150.274 1113.58 146.802 1116.68 Q143.33 1119.76 137.01 1119.76 Q134.834 1119.76 132.566 1119.39 Q130.32 1119.02 127.913 1118.28 L127.913 1113.58 Q129.996 1114.72 132.219 1115.27 Q134.441 1115.83 136.918 1115.83 Q140.922 1115.83 143.26 1113.72 Q145.598 1111.61 145.598 1108 Q145.598 1104.39 143.26 1102.28 Q140.922 1100.18 136.918 1100.18 Q135.043 1100.18 133.168 1100.59 Q131.316 1101.01 129.371 1101.89 L129.371 1084.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M62.9365 743.703 Q59.3254 743.703 57.4967 747.268 Q55.6912 750.81 55.6912 757.939 Q55.6912 765.046 57.4967 768.61 Q59.3254 772.152 62.9365 772.152 Q66.5707 772.152 68.3763 768.61 Q70.205 765.046 70.205 757.939 Q70.205 750.81 68.3763 747.268 Q66.5707 743.703 62.9365 743.703 M62.9365 739.999 Q68.7467 739.999 71.8022 744.606 Q74.8809 749.189 74.8809 757.939 Q74.8809 766.666 71.8022 771.272 Q68.7467 775.856 62.9365 775.856 Q57.1264 775.856 54.0477 771.272 Q50.9921 766.666 50.9921 757.939 Q50.9921 749.189 54.0477 744.606 Q57.1264 739.999 62.9365 739.999 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M83.0984 769.305 L87.9827 769.305 L87.9827 775.184 L83.0984 775.184 L83.0984 769.305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M98.2141 740.624 L116.57 740.624 L116.57 744.56 L102.496 744.56 L102.496 753.032 Q103.515 752.685 104.534 752.523 Q105.552 752.337 106.571 752.337 Q112.358 752.337 115.737 755.509 Q119.117 758.68 119.117 764.097 Q119.117 769.675 115.645 772.777 Q112.172 775.856 105.853 775.856 Q103.677 775.856 101.409 775.485 Q99.1632 775.115 96.7558 774.374 L96.7558 769.675 Q98.8391 770.809 101.061 771.365 Q103.284 771.921 105.76 771.921 Q109.765 771.921 112.103 769.814 Q114.441 767.708 114.441 764.097 Q114.441 760.485 112.103 758.379 Q109.765 756.273 105.76 756.273 Q103.885 756.273 102.01 756.689 Q100.159 757.106 98.2141 757.985 L98.2141 740.624 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M138.33 743.703 Q134.719 743.703 132.89 747.268 Q131.084 750.81 131.084 757.939 Q131.084 765.046 132.89 768.61 Q134.719 772.152 138.33 772.152 Q141.964 772.152 143.769 768.61 Q145.598 765.046 145.598 757.939 Q145.598 750.81 143.769 747.268 Q141.964 743.703 138.33 743.703 M138.33 739.999 Q144.14 739.999 147.195 744.606 Q150.274 749.189 150.274 757.939 Q150.274 766.666 147.195 771.272 Q144.14 775.856 138.33 775.856 Q132.519 775.856 129.441 771.272 Q126.385 766.666 126.385 757.939 Q126.385 749.189 129.441 744.606 Q132.519 739.999 138.33 739.999 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M63.9319 399.797 Q60.3208 399.797 58.4921 403.362 Q56.6865 406.904 56.6865 414.034 Q56.6865 421.14 58.4921 424.705 Q60.3208 428.246 63.9319 428.246 Q67.5661 428.246 69.3717 424.705 Q71.2004 421.14 71.2004 414.034 Q71.2004 406.904 69.3717 403.362 Q67.5661 399.797 63.9319 399.797 M63.9319 396.094 Q69.742 396.094 72.7976 400.7 Q75.8763 405.284 75.8763 414.034 Q75.8763 422.76 72.7976 427.367 Q69.742 431.95 63.9319 431.95 Q58.1217 431.95 55.043 427.367 Q51.9875 422.76 51.9875 414.034 Q51.9875 405.284 55.043 400.7 Q58.1217 396.094 63.9319 396.094 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M84.0938 425.399 L88.978 425.399 L88.978 431.279 L84.0938 431.279 L84.0938 425.399 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M97.9826 396.719 L120.205 396.719 L120.205 398.71 L107.658 431.279 L102.774 431.279 L114.58 400.654 L97.9826 400.654 L97.9826 396.719 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M129.371 396.719 L147.728 396.719 L147.728 400.654 L133.654 400.654 L133.654 409.126 Q134.672 408.779 135.691 408.617 Q136.709 408.432 137.728 408.432 Q143.515 408.432 146.894 411.603 Q150.274 414.774 150.274 420.191 Q150.274 425.77 146.802 428.871 Q143.33 431.95 137.01 431.95 Q134.834 431.95 132.566 431.58 Q130.32 431.209 127.913 430.469 L127.913 425.77 Q129.996 426.904 132.219 427.459 Q134.441 428.015 136.918 428.015 Q140.922 428.015 143.26 425.908 Q145.598 423.802 145.598 420.191 Q145.598 416.58 143.26 414.473 Q140.922 412.367 136.918 412.367 Q135.043 412.367 133.168 412.784 Q131.316 413.2 129.371 414.08 L129.371 396.719 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M53.7467 83.438 L61.3856 83.438 L61.3856 57.0724 L53.0754 58.7391 L53.0754 54.4798 L61.3393 52.8132 L66.0152 52.8132 L66.0152 83.438 L73.654 83.438 L73.654 87.3732 L53.7467 87.3732 L53.7467 83.438 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M83.0984 81.4936 L87.9827 81.4936 L87.9827 87.3732 L83.0984 87.3732 L83.0984 81.4936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M108.168 55.8919 Q104.557 55.8919 102.728 59.4567 Q100.922 62.9983 100.922 70.1279 Q100.922 77.2343 102.728 80.7991 Q104.557 84.3408 108.168 84.3408 Q111.802 84.3408 113.608 80.7991 Q115.436 77.2343 115.436 70.1279 Q115.436 62.9983 113.608 59.4567 Q111.802 55.8919 108.168 55.8919 M108.168 52.1882 Q113.978 52.1882 117.033 56.7946 Q120.112 61.378 120.112 70.1279 Q120.112 78.8547 117.033 83.4612 Q113.978 88.0445 108.168 88.0445 Q102.358 88.0445 99.2789 83.4612 Q96.2234 78.8547 96.2234 70.1279 Q96.2234 61.378 99.2789 56.7946 Q102.358 52.1882 108.168 52.1882 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M138.33 55.8919 Q134.719 55.8919 132.89 59.4567 Q131.084 62.9983 131.084 70.1279 Q131.084 77.2343 132.89 80.7991 Q134.719 84.3408 138.33 84.3408 Q141.964 84.3408 143.769 80.7991 Q145.598 77.2343 145.598 70.1279 Q145.598 62.9983 143.769 59.4567 Q141.964 55.8919 138.33 55.8919 M138.33 52.1882 Q144.14 52.1882 147.195 56.7946 Q150.274 61.378 150.274 70.1279 Q150.274 78.8547 147.195 83.4612 Q144.14 88.0445 138.33 88.0445 Q132.519 88.0445 129.441 83.4612 Q126.385 78.8547 126.385 70.1279 Q126.385 61.378 129.441 56.7946 Q132.519 52.1882 138.33 52.1882 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip842)\" d=\"\nM305.434 1436.09 L305.434 1445.72 L856.338 1445.72 L856.338 1436.09 L305.434 1436.09 L305.434 1436.09  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  305.434,1436.09 305.434,1445.72 856.338,1445.72 856.338,1436.09 305.434,1436.09 \n  \"/>\n<path clip-path=\"url(#clip842)\" d=\"\nM994.063 87.9763 L994.063 1445.72 L1544.97 1445.72 L1544.97 87.9763 L994.063 87.9763 L994.063 87.9763  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  994.063,87.9763 994.063,1445.72 1544.97,1445.72 1544.97,87.9763 994.063,87.9763 \n  \"/>\n<path clip-path=\"url(#clip842)\" d=\"\nM1682.69 1437.46 L1682.69 1445.72 L2233.6 1445.72 L2233.6 1437.46 L1682.69 1437.46 L1682.69 1437.46  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip842)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1682.69,1437.46 1682.69,1445.72 2233.6,1445.72 2233.6,1437.46 1682.69,1437.46 \n  \"/>\n<path clip-path=\"url(#clip840)\" d=\"\nM1694.01 198.898 L2280.54 198.898 L2280.54 95.2176 L1694.01 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1694.01,198.898 2280.54,198.898 2280.54,95.2176 1694.01,95.2176 1694.01,198.898 \n  \"/>\n<path clip-path=\"url(#clip840)\" d=\"\nM1718.08 167.794 L1862.52 167.794 L1862.52 126.322 L1718.08 126.322 L1718.08 167.794  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip840)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1718.08,167.794 1862.52,167.794 1862.52,126.322 1718.08,126.322 1718.08,167.794 \n  \"/>\n<path clip-path=\"url(#clip840)\" d=\"M1890.87 160.449 L1890.87 174.199 L1886.59 174.199 L1886.59 138.412 L1890.87 138.412 L1890.87 142.347 Q1892.21 140.032 1894.25 138.921 Q1896.31 137.787 1899.16 137.787 Q1903.88 137.787 1906.82 141.537 Q1909.78 145.287 1909.78 151.398 Q1909.78 157.509 1906.82 161.259 Q1903.88 165.009 1899.16 165.009 Q1896.31 165.009 1894.25 163.898 Q1892.21 162.763 1890.87 160.449 M1905.36 151.398 Q1905.36 146.699 1903.42 144.037 Q1901.49 141.352 1898.12 141.352 Q1894.74 141.352 1892.79 144.037 Q1890.87 146.699 1890.87 151.398 Q1890.87 156.097 1892.79 158.782 Q1894.74 161.444 1898.12 161.444 Q1901.49 161.444 1903.42 158.782 Q1905.36 156.097 1905.36 151.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1916.84 138.412 L1921.1 138.412 L1921.1 164.338 L1916.84 164.338 L1916.84 138.412 M1916.84 128.319 L1921.1 128.319 L1921.1 133.713 L1916.84 133.713 L1916.84 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1948.67 139.407 L1948.67 143.389 Q1946.86 142.393 1945.04 141.907 Q1943.23 141.398 1941.38 141.398 Q1937.24 141.398 1934.94 144.037 Q1932.65 146.652 1932.65 151.398 Q1932.65 156.143 1934.94 158.782 Q1937.24 161.398 1941.38 161.398 Q1943.23 161.398 1945.04 160.912 Q1946.86 160.402 1948.67 159.407 L1948.67 163.342 Q1946.89 164.176 1944.97 164.592 Q1943.07 165.009 1940.92 165.009 Q1935.06 165.009 1931.61 161.328 Q1928.16 157.648 1928.16 151.398 Q1928.16 145.055 1931.63 141.421 Q1935.13 137.787 1941.19 137.787 Q1943.16 137.787 1945.04 138.203 Q1946.91 138.597 1948.67 139.407 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M1955.92 128.319 L1960.2 128.319 L1960.2 149.592 L1972.91 138.412 L1978.35 138.412 L1964.6 150.541 L1978.92 164.338 L1973.37 164.338 L1960.2 151.676 L1960.2 164.338 L1955.92 164.338 L1955.92 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2004.02 150.31 L2004.02 152.393 L1984.43 152.393 Q1984.71 156.791 1987.07 159.106 Q1989.46 161.398 1993.69 161.398 Q1996.15 161.398 1998.44 160.796 Q2000.75 160.194 2003.02 158.99 L2003.02 163.018 Q2000.73 163.99 1998.32 164.5 Q1995.92 165.009 1993.44 165.009 Q1987.24 165.009 1983.6 161.398 Q1979.99 157.787 1979.99 151.629 Q1979.99 145.264 1983.42 141.537 Q1986.86 137.787 1992.7 137.787 Q1997.93 137.787 2000.96 141.166 Q2004.02 144.523 2004.02 150.31 M1999.76 149.06 Q1999.71 145.565 1997.79 143.481 Q1995.89 141.398 1992.74 141.398 Q1989.18 141.398 1987.03 143.412 Q1984.9 145.426 1984.57 149.083 L1999.76 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2028.07 142.347 L2028.07 128.319 L2032.33 128.319 L2032.33 164.338 L2028.07 164.338 L2028.07 160.449 Q2026.73 162.763 2024.67 163.898 Q2022.63 165.009 2019.76 165.009 Q2015.06 165.009 2012.1 161.259 Q2009.16 157.509 2009.16 151.398 Q2009.16 145.287 2012.1 141.537 Q2015.06 137.787 2019.76 137.787 Q2022.63 137.787 2024.67 138.921 Q2026.73 140.032 2028.07 142.347 M2013.55 151.398 Q2013.55 156.097 2015.48 158.782 Q2017.42 161.444 2020.8 161.444 Q2024.18 161.444 2026.12 158.782 Q2028.07 156.097 2028.07 151.398 Q2028.07 146.699 2026.12 144.037 Q2024.18 141.352 2020.8 141.352 Q2017.42 141.352 2015.48 144.037 Q2013.55 146.699 2013.55 151.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2069.29 128.319 L2069.29 131.861 L2065.22 131.861 Q2062.93 131.861 2062.03 132.787 Q2061.15 133.713 2061.15 136.12 L2061.15 138.412 L2068.16 138.412 L2068.16 141.722 L2061.15 141.722 L2061.15 164.338 L2056.86 164.338 L2056.86 141.722 L2052.79 141.722 L2052.79 138.412 L2056.86 138.412 L2056.86 136.606 Q2056.86 132.278 2058.88 130.31 Q2060.89 128.319 2065.27 128.319 L2069.29 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2084.64 151.305 Q2079.48 151.305 2077.49 152.486 Q2075.5 153.666 2075.5 156.514 Q2075.5 158.782 2076.98 160.125 Q2078.48 161.444 2081.05 161.444 Q2084.6 161.444 2086.73 158.944 Q2088.88 156.421 2088.88 152.254 L2088.88 151.305 L2084.64 151.305 M2093.14 149.546 L2093.14 164.338 L2088.88 164.338 L2088.88 160.402 Q2087.42 162.763 2085.24 163.898 Q2083.07 165.009 2079.92 165.009 Q2075.94 165.009 2073.58 162.787 Q2071.24 160.541 2071.24 156.791 Q2071.24 152.416 2074.16 150.194 Q2077.1 147.972 2082.91 147.972 L2088.88 147.972 L2088.88 147.555 Q2088.88 144.615 2086.93 143.018 Q2085.01 141.398 2081.52 141.398 Q2079.29 141.398 2077.19 141.93 Q2075.08 142.463 2073.14 143.527 L2073.14 139.592 Q2075.48 138.69 2077.67 138.25 Q2079.87 137.787 2081.96 137.787 Q2087.58 137.787 2090.36 140.703 Q2093.14 143.62 2093.14 149.546 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2098.85 138.412 L2103.37 138.412 L2111.47 160.171 L2119.57 138.412 L2124.09 138.412 L2114.36 164.338 L2108.58 164.338 L2098.85 138.412 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2140.01 141.398 Q2136.59 141.398 2134.6 144.083 Q2132.6 146.745 2132.6 151.398 Q2132.6 156.051 2134.57 158.736 Q2136.56 161.398 2140.01 161.398 Q2143.41 161.398 2145.41 158.713 Q2147.4 156.027 2147.4 151.398 Q2147.4 146.791 2145.41 144.106 Q2143.41 141.398 2140.01 141.398 M2140.01 137.787 Q2145.57 137.787 2148.74 141.398 Q2151.91 145.009 2151.91 151.398 Q2151.91 157.764 2148.74 161.398 Q2145.57 165.009 2140.01 165.009 Q2134.43 165.009 2131.26 161.398 Q2128.11 157.764 2128.11 151.398 Q2128.11 145.009 2131.26 141.398 Q2134.43 137.787 2140.01 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2173.99 142.393 Q2173.28 141.977 2172.42 141.791 Q2171.59 141.583 2170.57 141.583 Q2166.96 141.583 2165.01 143.944 Q2163.09 146.282 2163.09 150.68 L2163.09 164.338 L2158.81 164.338 L2158.81 138.412 L2163.09 138.412 L2163.09 142.44 Q2164.43 140.078 2166.59 138.944 Q2168.74 137.787 2171.82 137.787 Q2172.26 137.787 2172.79 137.856 Q2173.32 137.903 2173.97 138.018 L2173.99 142.393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2178.46 138.412 L2182.72 138.412 L2182.72 164.338 L2178.46 164.338 L2178.46 138.412 M2178.46 128.319 L2182.72 128.319 L2182.72 133.713 L2178.46 133.713 L2178.46 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2195.85 131.051 L2195.85 138.412 L2204.62 138.412 L2204.62 141.722 L2195.85 141.722 L2195.85 155.796 Q2195.85 158.967 2196.7 159.87 Q2197.58 160.773 2200.24 160.773 L2204.62 160.773 L2204.62 164.338 L2200.24 164.338 Q2195.31 164.338 2193.44 162.509 Q2191.56 160.657 2191.56 155.796 L2191.56 141.722 L2188.44 141.722 L2188.44 138.412 L2191.56 138.412 L2191.56 131.051 L2195.85 131.051 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip840)\" d=\"M2232.4 150.31 L2232.4 152.393 L2212.81 152.393 Q2213.09 156.791 2215.45 159.106 Q2217.84 161.398 2222.07 161.398 Q2224.53 161.398 2226.82 160.796 Q2229.13 160.194 2231.4 158.99 L2231.4 163.018 Q2229.11 163.99 2226.7 164.5 Q2224.29 165.009 2221.82 165.009 Q2215.61 165.009 2211.98 161.398 Q2208.37 157.787 2208.37 151.629 Q2208.37 145.264 2211.79 141.537 Q2215.24 137.787 2221.08 137.787 Q2226.31 137.787 2229.34 141.166 Q2232.4 144.523 2232.4 150.31 M2228.14 149.06 Q2228.09 145.565 2226.17 143.481 Q2224.27 141.398 2221.12 141.398 Q2217.56 141.398 2215.41 143.412 Q2213.28 145.426 2212.95 149.083 L2228.14 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning about an agent from their actions: formalization\n",
    "\n",
    "We will now formalize the kind of inference in the previous example. We begin by considering inference over the utilities and softmax noise parameter for an MDP agent. Later on we'll generalize to POMDP agents and to other agents.\n",
    "\n",
    "Following [Chapter 3.1](/chapters/3a-mdp.html) the MDP agent is defined by a utility function $U$ and softmax parameter $\\alpha$. In order to do inference, we need to know the agent's starting state $s_0$ (which might include both their *location* and their *time horizon* $N$). The data we condition on is a sequence of state-action pairs:\n",
    "\n",
    "$\n",
    "(s_0, a_0), (s_1, a_1), \\ldots, (s_n, a_n)\n",
    "$\n",
    "\n",
    "The index for the final timestep is less than or equal to the time horzion:  $n \\leq N$. We abbreviate this sequence as $(s,a)_{0:n}$. The joint posterior on the agent's utilities and noise given the observed state-action sequence is:\n",
    "\n",
    "$\n",
    "P(U,\\alpha | (s,a)_{0:n}) \\propto P( {(s,a)}_{0:n} | U, \\alpha) P(U, \\alpha)\n",
    "$\n",
    "\n",
    "where the likelihood function $P( {(s,a)}_{0:n} \\vert U, \\alpha )$ is the MDP agent model (for simplicity we omit information about the starting state). Due to the Markov Assumption for MDPs, the probability of an agent's action in a state is independent of the agent's previous or later actions (given $U$ and $\\alpha$). This allows us to rewrite the posterior as **Equation (1)**:\n",
    "\n",
    "$\n",
    "P(U,\\alpha | (s,a)_{0:n}) \\propto P(U, \\alpha) \\prod_{i=0}^n P( a_i | s_i, U, \\alpha)\n",
    "$\n",
    "\n",
    "\n",
    "The term $P( a_i \\vert s_i, U, \\alpha)$ can be rewritten as the softmax choice function (which corresponds to the function `act` in our MDP agent models). This equation holds for the case where we observe a sequence of actions from timestep $0$ to $n \\leq N$ (with no gaps). This tutorial focuses mostly on this case. It is trivial to extend the equation to observing multiple independently drawn such sequences (as we show below). However, if there are gaps in the sequence or if we observe only the agent's states (not the actions), then we need to marginalize over actions that were unobserved.\n",
    "\n",
    "\n",
    "## Examples of learning about agents in MDPs\n",
    "\n",
    "### Example: Inference from part of a sequence of actions\n",
    "\n",
    "The expression for the joint posterior (above) shows that it is straightforward to do inference on a part of an agent's action sequence. For example, if we know an agent had a time horizon $N=11$, we can do inference from only the agent's first few actions.\n",
    "\n",
    "For this example we condition on the agent making a single step from $[3,1]$ to $[2,1]$ by moving left. For an agent with low noise, this already provides very strong evidence about the agent's preferences -- not much is added by seeing the agent go all the way to Donut South.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: restaurant constants\n",
    "var ___ = ' ';\n",
    "var DN = { name: 'Donut N' };\n",
    "var DS = { name: 'Donut S' };\n",
    "var V = { name: 'Veg' };\n",
    "var N = { name: 'Noodle' };\n",
    "///\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var world = makeGridWorldMDP({ grid }).world;\n",
    "\n",
    "var trajectory = [\n",
    "  {\n",
    "    loc: [3, 1],\n",
    "    timeLeft: 11,\n",
    "    terminateAfterAction: false\n",
    "  },\n",
    "  {\n",
    "    loc: [2, 1],\n",
    "    timeLeft: 10,\n",
    "    terminateAfterAction: false\n",
    "  }\n",
    "];\n",
    "\n",
    "viz.gridworld(world, { trajectory });\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our approach to inference is slightly different than in the example at the start of this chapter. The approach is a direct translation of the expression for the posterior in Equation (1) above. For each observed state-action pair, we compute the likelihood of the agent (with given $U$) choosing that action in the state. In contrast, the simple approach above becomes intractable for long, noisy action sequences -- as it will need to loop over all possible sequences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: create restaurant choice MDP\n",
    "var ___ = ' ';\n",
    "var DN = { name : 'Donut N' };\n",
    "var DS = { name : 'Donut S' };\n",
    "var V = { name : 'Veg' };\n",
    "var N = { name : 'Noodle' };\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  noReverse: true,\n",
    "  maxTimeAtRestaurant: 2,\n",
    "  start: [3, 1],\n",
    "  totalTime: 11\n",
    "});\n",
    "///\n",
    "\n",
    "var world = mdp.world;\n",
    "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
    "\n",
    "var utilityTablePrior = function(){\n",
    "  var baseUtilityTable = {\n",
    "    'Donut S': 1,\n",
    "    'Donut N': 1,\n",
    "    'Veg': 1,\n",
    "    'Noodle': 1,\n",
    "    'timeCost': -0.04\n",
    "  };\n",
    "  return uniformDraw(\n",
    "    [{ table: extend(baseUtilityTable, { 'Donut N': 2, 'Donut S': 2 }),\n",
    "       favourite: 'donut' },\n",
    "     { table: extend(baseUtilityTable, { 'Veg': 2 }),\n",
    "       favourite: 'veg' },\n",
    "     { table: extend(baseUtilityTable, { 'Noodle': 2 }),\n",
    "       favourite: 'noodle' }]\n",
    "  );\n",
    "};\n",
    "\n",
    "var observedTrajectory = [[{\n",
    "  loc: [3, 1],\n",
    "  timeLeft: 11,\n",
    "  terminateAfterAction: false\n",
    "}, 'l']];\n",
    "\n",
    "var posterior = Infer({ model() {\n",
    "  var utilityTableAndFavourite = utilityTablePrior();\n",
    "  var utilityTable = utilityTableAndFavourite.table;\n",
    "  var utility = makeUtilityFunction(utilityTable);\n",
    "  var favourite = utilityTableAndFavourite.favourite;\n",
    "\n",
    "  var agent  = makeMDPAgent({ utility, alpha: 2 }, world);\n",
    "  var act = agent.act;\n",
    "\n",
    "  // For each observed state-action pair, factor on likelihood of action\n",
    "  map(\n",
    "    function(stateAction){\n",
    "      var state = stateAction[0];\n",
    "      var action = stateAction[1];\n",
    "      observe(act(state), action);\n",
    "    },\n",
    "    observedTrajectory);\n",
    "\n",
    "  return { favourite };\n",
    "}});\n",
    "\n",
    "viz(posterior);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that utility functions where Veg or Noodle are most preferred have almost the same posterior probability. Since they had the same prior, this means that we haven't received evidence about which the agent prefers. Moreover, assuming the agent's `timeCost` is negligible, then no matter where the agent above starts out on the grid, they choose Donut North or South. So we never get any information about whether they prefer the Vegetarian Cafe or Noodle Shop!\n",
    "\n",
    "Actually, this is not quite right. If we wait long enough, the agent's softmax noise would eventually reveal information about which was preferred. However, we still won't be able to *efficiently* learn the agent's preferences by repeatedly watching them choose from a random start point. If there is no softmax noise, then we can make the stronger claim that even in the limit of arbitrarily many repeated i.i.d. observations, the agent's preferences are not *identified* by draws from this space of scenarios.\n",
    "\n",
    "Unidentifiability is a frequent problem when inferring an agent's beliefs or utilities from realistic datasets. First, agents with low noise reliably avoid inferior states (as in the present example) and so their actions provide little information about the relative utilities among the inferior states. Second, using richer agent models means there are more possible explanations of the same behavior. For example, agents with high softmax noise or with false beliefs might go to a restaurant even if they don't prefer it. One general approach to the problem of unidentifiability in IRL is **active learning**. Instead of passively observing the agent's actions, you select a sequence of environments that will be maximally informative about the agent's preferences. For recent work covering both the nature of unidentifiability in IRL as well as the active learning approach, see reft:amin2016towards.\n",
    "\n",
    "### Example: Inferring The Cost of Time and Softmax Noise\n",
    "\n",
    "The previous examples assumed that the agent's `timeCost` (the negative utility of each timestep before the agent reaches a restaurant) and the softmax $\\alpha$ were known. We can modify the above example to include them in inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "// infer_utilities_timeCost_softmax_noise\n",
    "///fold: create restaurant choice MDP, donutSouthTrajectory\n",
    "var ___ = ' ';\n",
    "var DN = { name : 'Donut N' };\n",
    "var DS = { name : 'Donut S' };\n",
    "var V = { name : 'Veg' };\n",
    "var N = { name : 'Noodle' };\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  noReverse: true,\n",
    "  maxTimeAtRestaurant: 2,\n",
    "  start: [3, 1],\n",
    "  totalTime: 11\n",
    "});\n",
    "\n",
    "var donutSouthTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"l\"],\n",
    "  [{\"loc\":[2,1],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"l\"],\n",
    "  [{\"loc\":[1,1],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[2,1]},\"l\"],\n",
    "  [{\"loc\":[0,1],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[1,1]},\"d\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[0,1],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":true,\"timeLeft\":7,\"previousLoc\":[0,0],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "\n",
    "var vegDirectTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"u\"],\n",
    "  [{\"loc\":[3,2],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"u\"],\n",
    "  [{\"loc\":[3,3],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[3,2]},\"u\"],\n",
    "  [{\"loc\":[3,4],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[3,3]},\"u\"],\n",
    "  [{\"loc\":[3,5],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[3,4]},\"u\"],\n",
    "  [{\"loc\":[3,6],\"terminateAfterAction\":false,\"timeLeft\":6,\"previousLoc\":[3,5]},\"r\"],\n",
    "  [{\"loc\":[4,6],\"terminateAfterAction\":false,\"timeLeft\":5,\"previousLoc\":[3,6]},\"u\"],\n",
    "  [{\"loc\":[4,7],\"terminateAfterAction\":false,\"timeLeft\":4,\"previousLoc\":[4,6],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[4,7],\"terminateAfterAction\":true,\"timeLeft\":4,\"previousLoc\":[4,7],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "///\n",
    "\n",
    "var world = mdp.world;\n",
    "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
    "\n",
    "\n",
    "// Priors\n",
    "\n",
    "var utilityTablePrior = function() {\n",
    "  var foodValues = [0, 1, 2];\n",
    "  var timeCostValues = [-0.1, -0.3, -0.6];\n",
    "  var donut = uniformDraw(foodValues);\n",
    "  return {\n",
    "    'Donut N': donut,\n",
    "    'Donut S': donut,\n",
    "    'Veg': uniformDraw(foodValues),\n",
    "    'Noodle': uniformDraw(foodValues),\n",
    "    'timeCost': uniformDraw(timeCostValues)\n",
    "  };\n",
    "};\n",
    "\n",
    "var alphaPrior = function(){\n",
    "  return uniformDraw([.1, 1, 10, 100]);\n",
    "};\n",
    "\n",
    "\n",
    "// Condition on observed trajectory\n",
    "\n",
    "var posterior = function(observedTrajectory){\n",
    "  return Infer({ model() {\n",
    "    var utilityTable = utilityTablePrior();\n",
    "    var alpha = alphaPrior();\n",
    "    var params = {\n",
    "      utility: makeUtilityFunction(utilityTable),\n",
    "      alpha\n",
    "    };\n",
    "    var agent = makeMDPAgent(params, world);\n",
    "    var act = agent.act;\n",
    "\n",
    "    // For each observed state-action pair, factor on likelihood of action\n",
    "    map(\n",
    "      function(stateAction){\n",
    "        var state = stateAction[0];\n",
    "        var action = stateAction[1]\n",
    "        observe(act(state), action);\n",
    "      },\n",
    "      observedTrajectory);\n",
    "\n",
    "    // Compute whether Donut is preferred to Veg and Noodle\n",
    "    var donut = utilityTable['Donut N'];\n",
    "    var donutFavorite = (\n",
    "      donut > utilityTable.Veg &&\n",
    "      donut > utilityTable.Noodle);\n",
    "\n",
    "    return {\n",
    "      donutFavorite,\n",
    "      alpha: alpha.toString(),\n",
    "      timeCost: utilityTable.timeCost.toString()\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "\n",
    "print('Prior:');\n",
    "var prior = posterior([]);\n",
    "viz.marginals(prior);\n",
    "\n",
    "print('Conditioning on one action:');\n",
    "var posterior = posterior(donutSouthTrajectory.slice(0, 1));\n",
    "viz.marginals(posterior);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The posterior shows that taking a step towards Donut South can now be explained in terms of a high `timeCost`. If the agent has a low value for $\\alpha$, this step to the left is fairly likely even if the agent prefers Noodle or Veg. So including softmax noise in the inference makes inferences about other parameters closer to the prior.\n",
    "\n",
    ">**Exercise:** Suppose the agent is observed going all the way to Veg. What would the posteriors on $\\alpha$ and `timeCost` look like? Check your answer by conditioning on the state-action sequence `vegDirectTrajectory`. You will need to modify other parts of the codebox above to make this work.\n",
    "\n",
    "As we noted previously, it is simple to extend our approach to inference to conditioning on multiple sequences of actions. Consider the two sequences below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: make restaurant choice MDP, naiveTrajectory, donutSouthTrajectory\n",
    "var ___ = ' ';\n",
    "var DN = { name : 'Donut N' };\n",
    "var DS = { name : 'Donut S' };\n",
    "var V = { name : 'Veg' };\n",
    "var N = { name : 'Noodle' };\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  noReverse: true,\n",
    "  maxTimeAtRestaurant: 2,\n",
    "  start: [3, 1],\n",
    "  totalTime: 11\n",
    "});\n",
    "\n",
    "var naiveTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"u\"],\n",
    "  [{\"loc\":[3,2],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"u\"],\n",
    "  [{\"loc\":[3,3],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[3,2]},\"u\"],\n",
    "  [{\"loc\":[3,4],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[3,3]},\"u\"],\n",
    "  [{\"loc\":[3,5],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[3,4]},\"l\"],\n",
    "  [{\"loc\":[2,5],\"terminateAfterAction\":false,\"timeLeft\":6,\"previousLoc\":[3,5],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[2,5],\"terminateAfterAction\":true,\"timeLeft\":6,\"previousLoc\":[2,5],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "\n",
    "var donutSouthTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"l\"],\n",
    "  [{\"loc\":[2,1],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"l\"],\n",
    "  [{\"loc\":[1,1],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[2,1]},\"l\"],\n",
    "  [{\"loc\":[0,1],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[1,1]},\"d\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[0,1],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":true,\"timeLeft\":7,\"previousLoc\":[0,0],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "///\n",
    "\n",
    "var world = mdp.world;;\n",
    "\n",
    "map(function(trajectory) { viz.gridworld(world, { trajectory }); },\n",
    "    [naiveTrajectory, donutSouthTrajectory]);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform inference, we just condition on both sequences. (We use concatenation but we could have taken the union of all state-action pairs)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: World and agent are exactly as above\n",
    "\n",
    "var ___ = ' ';\n",
    "var DN = { name : 'Donut N' };\n",
    "var DS = { name : 'Donut S' };\n",
    "var V = { name : 'Veg' };\n",
    "var N = { name : 'Noodle' };\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  noReverse: true,\n",
    "  maxTimeAtRestaurant: 2,\n",
    "  start: [3, 1],\n",
    "  totalTime: 11\n",
    "});\n",
    "\n",
    "var donutSouthTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"l\"],\n",
    "  [{\"loc\":[2,1],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"l\"],\n",
    "  [{\"loc\":[1,1],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[2,1]},\"l\"],\n",
    "  [{\"loc\":[0,1],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[1,1]},\"d\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[0,1],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[0,0],\"terminateAfterAction\":true,\"timeLeft\":7,\"previousLoc\":[0,0],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "\n",
    "\n",
    "var naiveTrajectory = [\n",
    "  [{\"loc\":[3,1],\"terminateAfterAction\":false,\"timeLeft\":11},\"u\"],\n",
    "  [{\"loc\":[3,2],\"terminateAfterAction\":false,\"timeLeft\":10,\"previousLoc\":[3,1]},\"u\"],\n",
    "  [{\"loc\":[3,3],\"terminateAfterAction\":false,\"timeLeft\":9,\"previousLoc\":[3,2]},\"u\"],\n",
    "  [{\"loc\":[3,4],\"terminateAfterAction\":false,\"timeLeft\":8,\"previousLoc\":[3,3]},\"u\"],\n",
    "  [{\"loc\":[3,5],\"terminateAfterAction\":false,\"timeLeft\":7,\"previousLoc\":[3,4]},\"l\"],\n",
    "  [{\"loc\":[2,5],\"terminateAfterAction\":false,\"timeLeft\":6,\"previousLoc\":[3,5],\"timeAtRestaurant\":0},\"l\"],\n",
    "  [{\"loc\":[2,5],\"terminateAfterAction\":true,\"timeLeft\":6,\"previousLoc\":[2,5],\"timeAtRestaurant\":1},\"l\"]\n",
    "];\n",
    "\n",
    "var world = mdp.world;\n",
    "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
    "\n",
    "\n",
    "// Priors\n",
    "\n",
    "var utilityTablePrior = function() {\n",
    "  var foodValues = [0, 1, 2];\n",
    "  var timeCostValues = [-0.1, -0.3, -0.6];\n",
    "  var donut = uniformDraw(foodValues);\n",
    "  return {\n",
    "    'Donut N': donut,\n",
    "    'Donut S': donut,\n",
    "    'Veg': uniformDraw(foodValues),\n",
    "    'Noodle': uniformDraw(foodValues),\n",
    "    'timeCost': uniformDraw(timeCostValues)\n",
    "  };\n",
    "};\n",
    "\n",
    "var alphaPrior = function(){\n",
    "  return uniformDraw([.1, 1, 10, 100]);\n",
    "};\n",
    "\n",
    "\n",
    "// Condition on observed trajectory\n",
    "\n",
    "var posterior = function(observedTrajectory){\n",
    "  return Infer({ model() {\n",
    "    var utilityTable = utilityTablePrior();\n",
    "    var alpha = alphaPrior();\n",
    "    var params = {\n",
    "      utility: makeUtilityFunction(utilityTable),\n",
    "      alpha\n",
    "    };\n",
    "    var agent = makeMDPAgent(params, world);\n",
    "    var act = agent.act;\n",
    "\n",
    "    // For each observed state-action pair, factor on likelihood of action\n",
    "    map(\n",
    "      function(stateAction){\n",
    "        var state = stateAction[0];\n",
    "        var action = stateAction[1]\n",
    "        observe(act(state), action);\n",
    "      },\n",
    "      observedTrajectory);\n",
    "\n",
    "    // Compute whether Donut is preferred to Veg and Noodle\n",
    "    var donut = utilityTable['Donut N'];\n",
    "    var donutFavorite = (\n",
    "      donut > utilityTable.Veg &&\n",
    "      donut > utilityTable.Noodle);\n",
    "\n",
    "    return {\n",
    "      donutFavorite,\n",
    "      alpha: alpha.toString(),\n",
    "      timeCost: utilityTable.timeCost.toString()\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "\n",
    "///\n",
    "print('Prior:');\n",
    "var prior = posterior([]);\n",
    "viz.marginals(prior);\n",
    "\n",
    "print('Posterior');\n",
    "var posterior = posterior(naiveTrajectory.concat(donutSouthTrajectory));\n",
    "viz.marginals(posterior);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning about agents in POMDPs\n",
    "\n",
    "### Formalization\n",
    "\n",
    "We can extend our approach to inference to deal with agents that solve POMDPs. One approach to inference is simply to generate full state-action sequences and compare them to the observed data. As we mentioned above, this approach becomes intractable in cases where noise (in transitions and actions) is high and sequences are long.\n",
    "\n",
    "Instead, we extend the approach in Equation (1) above. The first thing to notice is that Equation (1) has to be amended for POMDPs. In an MDP, actions are conditionally independent given the agent's parameters $U$ and $\\alpha$ and the state. For any pair of actions $a_{i}$ and $a_j$ and state $s_i$:\n",
    "\n",
    "$\n",
    "P(a_i \\vert a_j, s_i, U,\\alpha) = P(a_i \\vert s_i, U,\\alpha)\n",
    "$\n",
    "\n",
    "In a POMDP, actions are only rendered conditionally independent if we also condition on the agent's *belief*. So Equation (1) can only be extended to the case where we know the agent's belief at each timestep. This will be realistic in some applications and not others. It depends on whether the agent's *observations* are part of the data that is conditioned on. If so, the agent's belief can be computed at each timestep (assuming the agent's initial belief is known). If not, we have to marginalize over the possible observations, making for a more complex inference computation.\n",
    "\n",
    "Here is the extension of Equation (1) to the POMDP case, where we assume access to the agent's observations. <a id=\"pomdpDefine\"></a>Our goal is to compute a posterior on the parameters of the agent. These include $U$ and $\\alpha$ as before but also the agent's initial belief $b_0$.\n",
    "\n",
    "We observe a sequence of state-observation-action triples:\n",
    "\n",
    "$\n",
    "(s_0,o_0,a_0), (s_1,o_1,a_1), \\ldots, (s_n,o_n,a_n)\n",
    "$\n",
    "\n",
    "The index for the final timestep is at most the time horzion:  $n \\leq N$. The joint posterior on the agent's utilities and noise given the observed sequence is:\n",
    "\n",
    "$\n",
    "P(U,\\alpha, b_0 | (s,o,a)_{0:n}) \\propto P( (s,o,a)_{0:n} | U, \\alpha, b_0)P(U, \\alpha, b_0)\n",
    "$\n",
    "\n",
    "To produce a factorized form of this posterior analogous to Equation (1), we compute the sequence of agent beliefs. This is given by the recursive Bayesian belief update described in [Chapter 3.3](/chapters/3c-pomdp):\n",
    "\n",
    "$\n",
    "b_i = b_{i-1} \\vert s_i, o_i, a_{i-1}\n",
    "$\n",
    "\n",
    "$\n",
    "b_i(s_i) \\propto\n",
    "O(s_i,a_{i-1},o_i)\n",
    "\\sum_{s_i \\in S} { T(s_{i-1}, a_{i-1}, s_i) b_{i-1}(s_{i-1})}\n",
    "$\n",
    "\n",
    "The posterior can thus be written as **Equation (2)**: <a id=\"pomdpInfer\"></a>\n",
    "\n",
    "$\n",
    "P(U, \\alpha, b_0 | (s,o,a)_{0:n}) \\propto P(U, \\alpha, b_0) \\prod_{i=0}^n P( a_i | s_i, b_i, U, \\alpha)\n",
    "$\n",
    "\n",
    "\n",
    "### Application: Bandits\n",
    "\n",
    "To learn the preferences and beliefs of a POMDP agent we translate Equation (2) into WebPPL. In a later [chapter](/chapters/5e-joint-inference.html), we apply this to the Restaurant Choice problem. Here we focus on the Bandit problems introduced in the [previous chapter](/chapters/3c-pomdp).\n",
    "\n",
    "In the Bandit problems there is an unknown mapping from arms to non-numeric prizes (or distributions on such prizes) and the agent has preferences over these prizes. The agent tries out arms to discover the mapping and exploits the most promising arms. In the *inverse* problem, we get to observe the agent's actions. Unlike the agent, we already know the mapping from arms to prizes. However, we don't know the agent's preferences or the agent's prior about the mapping[^bandit].\n",
    "\n",
    "[^bandit]: If we did not know the mapping from arms to prizes, the inference problem would not change fundamentally. We get information about this mapping by observing the prizes the agent receives when pulling different arms.\n",
    "\n",
    "Often the agent's choices admit of multiple explanations. Recall the deterministic example in the previous chapter when (according to the agent's belief) `arm0` had the prize \"chocolate\" and `arm1` had either \"champagne\" or \"nothing\" (see also Figure 2 below). Suppose we observe the agent chosing `arm0` on the first of five trials. If we don't know the agent's utilities or beliefs, then this choice could be explained by either:\n",
    "\n",
    "(1). the agent's preference for chocolate over champagne, or\n",
    "\n",
    "(2). the agent's belief that `arm1` is very likely (e.g. 95%) to yield the \"nothing\" prize deterministically\n",
    "\n",
    "Given this choice by the agent, we won't be able to identify which of (1) and (2) is true because exploration becomes less valuable every trial (and there's only 5 trials total).\n",
    "\n",
    "The codeboxes below implements this example. The translation of Equation (2) is in the function `factorSequence`. This function iterates through the observed state-observation-action triples, updating the agent's belief at each timestep. It interleaves conditioning on an action (via `factor`) with computing the sequence of belief functions $b_i$. The variable names correspond as follows:\n",
    "\n",
    "- $b_0$ is `initialBelief` (an argument to `factorSequence`)\n",
    "\n",
    "- $s_i$ is `state`\n",
    "\n",
    "- $b_i$ is `nextBelief`\n",
    "\n",
    "- $a_i$ is `observedAction`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~ javascript\n",
    "var inferBeliefsAndPreferences = function(baseAgentParams, priorPrizeToUtility,\n",
    "                                          priorInitialBelief, bandit,\n",
    "                                          observedSequence) {\n",
    "\n",
    "  return Infer({ model() {\n",
    "\n",
    "    // 1. Sample utilities\n",
    "    var prizeToUtility = (priorPrizeToUtility ? sample(priorPrizeToUtility)\n",
    "                          : undefined);\n",
    "\n",
    "    // 2. Sample beliefs\n",
    "    var initialBelief = sample(priorInitialBelief);\n",
    "\n",
    "    // 3. Construct agent given utilities and beliefs\n",
    "    var newAgentParams = extend(baseAgentParams, { priorBelief: initialBelief });\n",
    "    var agent = makeBanditAgent(newAgentParams, bandit, 'belief', prizeToUtility);\n",
    "    var agentAct = agent.act;\n",
    "    var agentUpdateBelief = agent.updateBelief;\n",
    "\n",
    "    // 4. Condition on observations\n",
    "    var factorSequence = function(currentBelief, previousAction, timeIndex){\n",
    "      if (timeIndex < observedSequence.length) {\n",
    "        var state = observedSequence[timeIndex].state;\n",
    "        var observation = observedSequence[timeIndex].observation;\n",
    "        var nextBelief = agentUpdateBelief(currentBelief, observation, previousAction);\n",
    "        var nextActionDist = agentAct(nextBelief);\n",
    "        var observedAction = observedSequence[timeIndex].action;\n",
    "        factor(nextActionDist.score(observedAction));\n",
    "        factorSequence(nextBelief, observedAction, timeIndex + 1);\n",
    "      }\n",
    "    };\n",
    "    factorSequence(initialBelief,'noAction', 0);\n",
    "\n",
    "    return {\n",
    "      prizeToUtility,\n",
    "      priorBelief: initialBelief\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# TODO: write Gen and Julia code for the above code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with a very simple example. The agent is observed pulling `arm1` five times. The agent's prior is known and assigns equal weight to `arm1` yielding \"champagne\" and to it yielding \"nothing\". The true prize for `arm1` is \"champagne\" (see Figure 1).\n",
    "\n",
    "<img src=\"/assets/img/4-irl-bandit-1.png\" alt=\"diagram\" style=\"width: 500px;\"/>\n",
    "\n",
    "> **Figure 1:** Bandit problem where agent's prior is known. (The true state has the bold outline).\n",
    "\n",
    "From the observation, it's obvious that the agent prefers champagne. This is what we infer below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: inferBeliefsAndPreferences, getMarginal\n",
    "var inferBeliefsAndPreferences = function(baseAgentParams, priorPrizeToUtility,\n",
    "                                          priorInitialBelief, bandit,\n",
    "                                          observedSequence) {\n",
    "\n",
    "  return Infer({ model() {\n",
    "\n",
    "    // 1. Sample utilities\n",
    "    var prizeToUtility = (priorPrizeToUtility ? sample(priorPrizeToUtility)\n",
    "                          : undefined);\n",
    "\n",
    "    // 2. Sample beliefs\n",
    "    var initialBelief = sample(priorInitialBelief);\n",
    "\n",
    "    // 3. Construct agent given utilities and beliefs\n",
    "    var newAgentParams = extend(baseAgentParams, { priorBelief: initialBelief });\n",
    "    var agent = makeBanditAgent(newAgentParams, bandit, 'belief', prizeToUtility);\n",
    "    var agentAct = agent.act;\n",
    "    var agentUpdateBelief = agent.updateBelief;\n",
    "\n",
    "    // 4. Condition on observations\n",
    "    var factorSequence = function(currentBelief, previousAction, timeIndex){\n",
    "      if (timeIndex < observedSequence.length) {\n",
    "        var state = observedSequence[timeIndex].state;\n",
    "        var observation = observedSequence[timeIndex].observation;\n",
    "        var nextBelief = agentUpdateBelief(currentBelief, observation, previousAction);\n",
    "        var nextActionDist = agentAct(nextBelief);\n",
    "        var observedAction = observedSequence[timeIndex].action;\n",
    "        factor(nextActionDist.score(observedAction));\n",
    "        factorSequence(nextBelief, observedAction, timeIndex + 1);\n",
    "      }\n",
    "    };\n",
    "    factorSequence(initialBelief,'noAction', 0);\n",
    "\n",
    "    return {\n",
    "      prizeToUtility,\n",
    "      priorBelief: initialBelief\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "\n",
    "var getMarginal = function(dist, key){\n",
    "  return Infer({ model() {\n",
    "    return sample(dist)[key];\n",
    "  }});\n",
    "};\n",
    "///\n",
    "// true prizes for arms\n",
    "var trueArmToPrizeDist = {\n",
    "  0: Delta({ v: 'chocolate' }),\n",
    "  1: Delta({ v: 'champagne' })\n",
    "};\n",
    "var bandit = makeBanditPOMDP({\n",
    "  armToPrizeDist: trueArmToPrizeDist,\n",
    "  numberOfArms: 2,\n",
    "  numberOfTrials: 5\n",
    "});\n",
    "\n",
    "// simpleAgent always pulls arm 1\n",
    "var simpleAgent = makePOMDPAgent({\n",
    "  act: function(belief){\n",
    "    return Infer({ model() { return 1; }});\n",
    "  },\n",
    "  updateBelief: function(belief){ return belief; },\n",
    "  params: { priorBelief: Delta({ v: bandit.startState }) }\n",
    "}, bandit.world);\n",
    "\n",
    "var observedSequence = simulatePOMDP(bandit.startState, bandit.world, simpleAgent,\n",
    "                                    'stateObservationAction');\n",
    "\n",
    "// Priors for inference\n",
    "\n",
    "// We know agent's prior, which is that either arm1 yields\n",
    "// nothing or it yields champagne.\n",
    "var priorInitialBelief = Delta({ v: Infer({ model() {\n",
    "  var armToPrizeDist = uniformDraw([\n",
    "    trueArmToPrizeDist,\n",
    "    extend(trueArmToPrizeDist, { 1: Delta({ v: 'nothing' }) })]);\n",
    "  return makeBanditStartState(5, armToPrizeDist);\n",
    "}})});\n",
    "\n",
    "// Agent either prefers chocolate or champagne.\n",
    "var likesChampagne = {\n",
    "  nothing: 0,\n",
    "  champagne: 5,\n",
    "  chocolate: 3\n",
    "};\n",
    "var likesChocolate = {\n",
    "  nothing: 0,\n",
    "  champagne: 3,\n",
    "  chocolate: 5\n",
    "};\n",
    "var priorPrizeToUtility = Categorical({\n",
    "  vs: [likesChampagne, likesChocolate],\n",
    "  ps: [0.5, 0.5]\n",
    "});\n",
    "var baseParams = { alpha: 1000 };\n",
    "var posterior = inferBeliefsAndPreferences(baseParams, priorPrizeToUtility,\n",
    "                                           priorInitialBelief, bandit,\n",
    "                                           observedSequence);\n",
    "\n",
    "print(\"After observing agent choose arm1, what are agent's utilities?\");\n",
    "print('Posterior on agent utilities:');\n",
    "viz.table(getMarginal(posterior, 'prizeToUtility'));\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the codebox above, the agent's preferences are identified by the observations. This won't hold for the next example, which we introduced previously. The agent's utilities for prizes are still unknown and now the agent's prior is also unknown. Either the agent is \"informed\" and knows the truth that `arm1` yields \"champagne\". Or the agent is misinformed and believes `arm1` is likely to yield \"nothing\". These two possibilities are depicted in Figure 2.\n",
    "\n",
    "<img src=\"/assets/img/4-irl-bandit-2.png\" alt=\"diagram\" style=\"width: 600px;\"/>\n",
    "\n",
    "> **Figure 2:** Bandit where agent's prior is unknown. The two large boxes depict the prior on the agent's initial belief. Each possibility for the agent's initial belief has probability 0.5.\n",
    "\n",
    "We observe the agent's first action, which is pulling `arm0`. Our inference approach is the same as above:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold:\n",
    "var inferBeliefsAndPreferences = function(baseAgentParams, priorPrizeToUtility,\n",
    "                                          priorInitialBelief, bandit,\n",
    "                                          observedSequence) {\n",
    "\n",
    "  return Infer({ model() {\n",
    "\n",
    "    // 1. Sample utilities\n",
    "    var prizeToUtility = (priorPrizeToUtility ? sample(priorPrizeToUtility)\n",
    "                          : undefined);\n",
    "\n",
    "    // 2. Sample beliefs\n",
    "    var initialBelief = sample(priorInitialBelief);\n",
    "\n",
    "    // 3. Construct agent given utilities and beliefs\n",
    "    var newAgentParams = extend(baseAgentParams, { priorBelief: initialBelief });\n",
    "    var agent = makeBanditAgent(newAgentParams, bandit, 'belief', prizeToUtility);\n",
    "    var agentAct = agent.act;\n",
    "    var agentUpdateBelief = agent.updateBelief;\n",
    "\n",
    "    // 4. Condition on observations\n",
    "    var factorSequence = function(currentBelief, previousAction, timeIndex){\n",
    "      if (timeIndex < observedSequence.length) {\n",
    "        var state = observedSequence[timeIndex].state;\n",
    "        var observation = observedSequence[timeIndex].observation;\n",
    "        var nextBelief = agentUpdateBelief(currentBelief, observation, previousAction);\n",
    "        var nextActionDist = agentAct(nextBelief);\n",
    "        var observedAction = observedSequence[timeIndex].action;\n",
    "        factor(nextActionDist.score(observedAction));\n",
    "        factorSequence(nextBelief, observedAction, timeIndex + 1);\n",
    "      }\n",
    "    };\n",
    "    factorSequence(initialBelief,'noAction', 0);\n",
    "\n",
    "    return {\n",
    "      prizeToUtility,\n",
    "      priorBelief: initialBelief\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "///\n",
    "var trueArmToPrizeDist = {\n",
    "  0: Delta({ v: 'chocolate' }),\n",
    "  1: Delta({ v: 'champagne' })\n",
    "};\n",
    "var bandit = makeBanditPOMDP({\n",
    "  numberOfArms: 2,\n",
    "  armToPrizeDist: trueArmToPrizeDist,\n",
    "  numberOfTrials: 5\n",
    "});\n",
    "\n",
    "var simpleAgent = makePOMDPAgent({\n",
    "  // simpleAgent always pulls arm 0\n",
    "  act: function(belief){\n",
    "    return Infer({ model() { return 0; }});\n",
    "  },\n",
    "  updateBelief: function(belief){ return belief; },\n",
    "  params: { priorBelief: Delta({ v: bandit.startState }) }\n",
    "}, bandit.world);\n",
    "\n",
    "var observedSequence = simulatePOMDP(bandit.startState, bandit.world, simpleAgent,\n",
    "                                     'stateObservationAction');\n",
    "\n",
    "// Agent either knows that arm1 has prize \"champagne\"\n",
    "// or agent thinks prize is probably \"nothing\"\n",
    "\n",
    "var informedPrior = Delta({ v: bandit.startState });\n",
    "var noChampagnePrior = Infer({ model() {\n",
    "  var armToPrizeDist = categorical(\n",
    "    [0.05, 0.95],\n",
    "    [trueArmToPrizeDist,\n",
    "     extend(trueArmToPrizeDist, { 1: Delta({ v: 'nothing' }) })]);\n",
    "  return makeBanditStartState(5, armToPrizeDist);\n",
    "}});\n",
    "\n",
    "var priorInitialBelief = Categorical({\n",
    "  vs: [informedPrior, noChampagnePrior],\n",
    "  ps: [0.5, 0.5]\n",
    "});\n",
    "\n",
    "// We are still uncertain about whether agent prefers chocolate or champagne\n",
    "var likesChampagne = {\n",
    "  nothing: 0,\n",
    "  champagne: 5,\n",
    "  chocolate: 3\n",
    "};\n",
    "var likesChocolate = {\n",
    "  nothing: 0,\n",
    "  champagne: 3,\n",
    "  chocolate: 5\n",
    "};\n",
    "\n",
    "var priorPrizeToUtility = Categorical({\n",
    "  ps: [0.5, 0.5],\n",
    "  vs: [likesChampagne, likesChocolate]\n",
    "});\n",
    "\n",
    "var baseParams = {alpha: 1000};\n",
    "var posterior = inferBeliefsAndPreferences(baseParams, priorPrizeToUtility,\n",
    "                                           priorInitialBelief, bandit,\n",
    "                                           observedSequence);\n",
    "\n",
    "var utilityBeliefPosterior = Infer({ model() {\n",
    "  var utilityBelief = sample(posterior);\n",
    "  var chocolateUtility = utilityBelief.prizeToUtility.chocolate;\n",
    "  var likesChocolate = chocolateUtility > 3;\n",
    "  var isInformed = utilityBelief.priorBelief.support().length === 1;\n",
    "  return { likesChocolate, isInformed };\n",
    "}});\n",
    "\n",
    "viz.table(utilityBeliefPosterior);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploration is more valuable if there are more Bandit trials in total. If we observe the agent choosing the arm they already know about (`arm0`) then we get stronger inferences about their preference for chocolate over champagne as the total trials increases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~ javascript\n",
    "// TODO simplify the code here or merge with previous example.\n",
    "///fold:\n",
    "var inferBeliefsAndPreferences = function(baseAgentParams, priorPrizeToUtility,\n",
    "                                           priorInitialBelief, bandit,\n",
    "                                           observedSequence) {\n",
    "\n",
    "  return Infer({ model() {\n",
    "\n",
    "    // 1. Sample utilities\n",
    "    var prizeToUtility = (priorPrizeToUtility ? sample(priorPrizeToUtility)\n",
    "                          : undefined);\n",
    "\n",
    "    // 2. Sample beliefs\n",
    "    var initialBelief = sample(priorInitialBelief);\n",
    "\n",
    "    // 3. Construct agent given utilities and beliefs\n",
    "    var newAgentParams = extend(baseAgentParams, { priorBelief: initialBelief });\n",
    "    var agent = makeBanditAgent(newAgentParams, bandit, 'belief', prizeToUtility);\n",
    "    var agentAct = agent.act;\n",
    "    var agentUpdateBelief = agent.updateBelief;\n",
    "\n",
    "    // 4. Condition on observations\n",
    "    var factorSequence = function(currentBelief, previousAction, timeIndex){\n",
    "      if (timeIndex < observedSequence.length) {\n",
    "        var state = observedSequence[timeIndex].state;\n",
    "        var observation = observedSequence[timeIndex].observation;\n",
    "        var nextBelief = agentUpdateBelief(currentBelief, observation, previousAction);\n",
    "        var nextActionDist = agentAct(nextBelief);\n",
    "        var observedAction = observedSequence[timeIndex].action;\n",
    "        factor(nextActionDist.score(observedAction));\n",
    "        factorSequence(nextBelief, observedAction, timeIndex + 1);\n",
    "      }\n",
    "    };\n",
    "    factorSequence(initialBelief,'noAction', 0);\n",
    "\n",
    "    return {\n",
    "      prizeToUtility,\n",
    "      priorBelief: initialBelief\n",
    "    };\n",
    "  }});\n",
    "};\n",
    "///\n",
    "\n",
    "var probLikesChocolate = function(numberOfTrials){\n",
    "  var trueArmToPrizeDist = {\n",
    "    0: Delta({ v: 'chocolate' }),\n",
    "    1: Delta({ v: 'champagne' })\n",
    "  };\n",
    "  var bandit = makeBanditPOMDP({\n",
    "    numberOfArms: 2,\n",
    "    armToPrizeDist: trueArmToPrizeDist,\n",
    "    numberOfTrials\n",
    "  });\n",
    "\n",
    "  var simpleAgent = makePOMDPAgent({\n",
    "    // simpleAgent always pulls arm 0\n",
    "    act: function(belief){\n",
    "      return Infer({ model() { return 0; }});\n",
    "    },\n",
    "    updateBelief: function(belief){ return belief; },\n",
    "    params: { priorBelief: Delta({ v: bandit.startState }) }\n",
    "  }, bandit.world);\n",
    "\n",
    "  var observedSequence = simulatePOMDP(bandit.startState, bandit.world, simpleAgent,\n",
    "                                       'stateObservationAction');\n",
    "\n",
    "  var baseParams = { alpha: 100 };\n",
    "\n",
    "  var noChampagnePrior = Infer({ model() {\n",
    "    var armToPrizeDist = (\n",
    "      flip(0.2) ?\n",
    "      trueArmToPrizeDist :\n",
    "      extend(trueArmToPrizeDist, { 1: Delta({ v: 'nothing' }) }));\n",
    "    return makeBanditStartState(numberOfTrials, armToPrizeDist);\n",
    "  }});\n",
    "  var informedPrior = Delta({ v: bandit.startState });\n",
    "  var priorInitialBelief = Categorical({\n",
    "    vs: [noChampagnePrior, informedPrior],\n",
    "    ps: [0.5, 0.5],\n",
    "  });\n",
    "\n",
    "  var likesChampagne = {\n",
    "    nothing: 0,\n",
    "    champagne: 5,\n",
    "    chocolate: 3\n",
    "  };\n",
    "  var likesChocolate = {\n",
    "    nothing: 0,\n",
    "    champagne: 3,\n",
    "    chocolate: 5\n",
    "  };\n",
    "\n",
    "  var priorPrizeToUtility = Categorical({\n",
    "    vs: [likesChampagne, likesChocolate],\n",
    "    ps: [0.5, 0.5],\n",
    "  });\n",
    "\n",
    "  var posterior = inferBeliefsAndPreferences(baseParams, priorPrizeToUtility,\n",
    "                                             priorInitialBelief, bandit,\n",
    "                                             observedSequence);\n",
    "\n",
    "  var likesChocInformed = {\n",
    "    prizeToUtility: likesChocolate,\n",
    "    priorBelief: informedPrior\n",
    "  };\n",
    "  var probLikesChocInformed = Math.exp(posterior.score(likesChocInformed));\n",
    "  var likesChocNoChampagne = {\n",
    "    prizeToUtility: likesChocolate,\n",
    "    priorBelief: noChampagnePrior\n",
    "  };\n",
    "  var probLikesChocNoChampagne = Math.exp(posterior.score(likesChocNoChampagne));\n",
    "  return probLikesChocInformed + probLikesChocNoChampagne;\n",
    "};\n",
    "\n",
    "var lifetimes = [5, 6, 7, 8, 9];\n",
    "var probsLikesChoc = map(probLikesChocolate, lifetimes);\n",
    "\n",
    "print('Probability of liking chocolate for lifetimes ' + lifetimes + '\\n'\n",
    "      + probsLikesChoc);\n",
    "\n",
    "viz.bar(lifetimes, probsLikesChoc);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example of inferring an agent's utilities from a Bandit problem may seem contrived. However, there are practical problems that have a similar structure. Consider a domain where $k$ **sources** (arms) produce a stream of content, with each piece of content having a **category** (prizes). At each timestep, a human is observed choosing a source. The human has uncertainty about the stochastic mapping from sources to categories. Our goal is to infer the human's beliefs about the sources and their preferences over categories. The sources could be blogs or feeds that tag posts using the same set of tags. Alternatively, the sources could be channels for TV shows or songs. In this kind of application, the same issue of identifiability arises. An agent may choose a source either because they know it produces content in the best categories or because they have a strong prior belief that it does.\n",
    "\n",
    "In the next [chapter](/chapters/5-biases-intro.html), we start looking at agents with cognitive bounds and biases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Footnotes\n",
    "\n",
    "[^generative]: The approach in economics closest to the one we outline here (with models of action based on sequential decision making) is called \"Structural Estimation\". Some particular examples are reft:aguirregabiria2010dynamic and reft:darden2010smoking. A related piece of work in AI or computational social science is reft:ermon2014learning.\n",
    "\n",
    "[^inverse]: The relevant papers on applications of IRL: parking cars in reft:abbeel2008apprenticeship, flying helicopters in reft:abbeel2010autonomous, controlling videogame bots in reft:lee2010learning, and table tennis in reft:muelling2014learning."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "file_extension": ".jl",
   "name": "julia",
   "mimetype": "application/julia",
   "version": "1.6.1"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.1",
   "language": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}