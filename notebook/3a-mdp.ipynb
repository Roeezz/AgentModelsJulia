{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "using Gen, Statistics, StatsPlots, BenchmarkTools, TimerOutputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "The previous chapter introduced agent models for solving simple, one-shot decision problems. The next few sections introduce *sequential* problems, where an agent's choice of action *now* depends on the actions they will choose in the future. As in game theory, the decision maker must coordinate with another rational agent. But in sequential decision problems, that rational agent is their future self.\n",
    "\n",
    "As a simple illustration of a sequential decision problem, suppose that an agent, Bob, is looking for a place to eat. Bob gets out of work in a particular location (indicated below by the blue circle). He knows the streets and the restaurants nearby. His decision problem is to take a sequence of actions such that (a) he eats at a restaurant he likes and (b) he does not spend too much time walking. Here is a visualization of the street layout. The labels refer to different types of restaurants: a chain selling Donuts, a Vegetarian Salad Bar and a Noodle Shop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: ask David about visualization of gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "var ___ = ' '; \n",
    "var DN = { name: 'Donut N' };\n",
    "var DS = { name: 'Donut S' };\n",
    "var V = { name: 'Veg' };\n",
    "var N = { name: 'Noodle' };\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({ grid, start: [3, 1] });\n",
    "\n",
    "viz.gridworld(mdp.world, { trajectory : [mdp.startState] });\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO: write above code in julia and gen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"mdp\"></a>\n",
    "\n",
    "## Markov Decision Processes: Definition\n",
    "\n",
    "We represent Bob's decision problem as a Markov Decision Process (MDP) and, more specifically, as a discrete \"Gridworld\" environment. An MDP is a tuple $ \\left\\langle S,A(s),T(s,a),U(s,a) \\right\\rangle$, including the *states*, the *actions* in each state, the *transition function* that maps state-action pairs to successor states, and the *utility* or *reward* function. In our example, the states $S$ are Bob's locations on the grid. At each state, Bob selects an action $a \\in \\{ \\text{up}, \\text{down}, \\text{left}, \\text{right} \\} $, which moves Bob around the grid (according to transition function $T$). In this example we assume that Bob's actions, as well as the transitions and utilities, are all deterministic. However, our approach generalizes to noisy actions, stochastic transitions and stochastic utilities.\n",
    "\n",
    "As with the one-shot decisions of the previous chapter, the agent in an MDP will choose actions that *maximize expected utility*. This depends on the total utility of the *sequence* of states that the agent visits. Formally, let $EU_{s}[a]$ be the expected (total) utility of action $a$ in state $s$. The agent's choice is a softmax function of this expected utility:\n",
    "\n",
    "$\n",
    "C(a; s) \\propto e^{\\alpha EU_{s}[a]}\n",
    "$\n",
    "\n",
    "The expected utility depends on both immediate utility and, recursively, on future expected utility:\n",
    "\n",
    "<a id=\"recursion\">**Expected Utility Recursion**</a>:\n",
    "\n",
    "$\n",
    "EU_{s}[a] = U(s, a) + \\mathbb{E}_{s', a'}(EU_{s'}[a'])\n",
    "$\n",
    "\n",
    "<br>\n",
    "with the next state $s' \\sim T(s,a)$ and $a' \\sim C(s')$. The decision problem ends either when a *terminal* state is reached or when the time-horizon is reached. (In the next few chapters the time-horizon will always be finite). \n",
    "\n",
    "The intuition to keep in mind for solving MDPs is that the expected utility propagates backwards from future states to the current action. If a high utility state can be reached by a sequence of actions starting from action $a$, then action $a$ will have high expected utility -- *provided* that the sequence of actions is taken with high probability and there are no low utility steps along the way."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Markov Decision Processes: Implementation\n",
    "\n",
    "The recursive decision rule for MDP agents can be directly translated into WebPPL. The `act` function takes the agent's state as input, evaluates the expectation of actions in that state, and returns a softmax distribution over actions. The expected utility of actions is computed by a separate function `expectedUtility`. Since an action's expected utility depends on future actions, `expectedUtility` calls `act` in a mutual recursion, bottoming out when a terminal state is reached or when time runs out. \n",
    "\n",
    "We illustrate this \"MDP agent\" on a simple MDP:\n",
    "\n",
    "### Integer Line MDP\n",
    "- **States**: Points on the integer line (e.g -1, 0, 1, 2).\n",
    "\n",
    "- **Actions/transitions**: Actions \"left\", \"right\" and \"stay\" move the agent deterministically along the line in either direction.\n",
    "\n",
    "- **Utility**: The utility is $1$ for the state corresponding to the integer $3$ and is $0$ otherwise. \n",
    "\n",
    "\n",
    "Here is a WebPPL agent that starts at the origin (`state === 0`) and that takes a first step (to the right):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "var transition = function(state, action) {\n",
    "  return state + action;\n",
    "};\n",
    "\n",
    "var utility = function(state) {\n",
    "  if (state === 3) {\n",
    "    return 1;\n",
    "  } else {\n",
    "    return 0;\n",
    "  }\n",
    "};\n",
    "\n",
    "var makeAgent = function() { \n",
    "  \n",
    "  var act = function(state, timeLeft) {\n",
    "    return Infer({ model() {\n",
    "      var action = uniformDraw([-1, 0, 1]);\n",
    "      var eu = expectedUtility(state, action, timeLeft);\n",
    "      factor(100 * eu);\n",
    "      return action;\n",
    "    }});\n",
    "  };\n",
    "\n",
    "  var expectedUtility = function(state, action, timeLeft){\n",
    "    var u = utility(state, action);\n",
    "    var newTimeLeft = timeLeft - 1;\n",
    "    if (newTimeLeft === 0){\n",
    "      return u; \n",
    "    } else {\n",
    "      return u + expectation(Infer({ model() {\n",
    "        var nextState = transition(state, action); \n",
    "        var nextAction = sample(act(nextState, newTimeLeft));\n",
    "        return expectedUtility(nextState, nextAction, newTimeLeft);\n",
    "      }}));\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return { act };\n",
    "}\n",
    "\n",
    "var act = makeAgent().act;\n",
    "\n",
    "var startState = 0;\n",
    "var totalTime = 4;\n",
    "\n",
    "// Agent's move '-1' means 'left', '0' means 'stay', '1' means 'right'\n",
    "print(\"Agent's action: \" + sample(act(startState, totalTime)));\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "function transition(state,action)\n",
    "    return state+action\n",
    "end\n",
    "\n",
    "function utility(state)\n",
    "  if state == 3\n",
    "    return 1\n",
    "  else\n",
    "    return 0\n",
    "  end\n",
    "end\n",
    "\n",
    "function make_agent()\n",
    "    @gen function act(state,time_left)\n",
    "        action = @trace(uniform_discrete(-1,1),:action)\n",
    "        eu = expected_utility(state,action,time_left)\n",
    "        @trace(bernoulli(exp(100 * eu)),:factor)\n",
    "    end\n",
    "    function expected_utility(state,action,time_left)\n",
    "        u = utility(state)\n",
    "        new_time_left = time_left-1\n",
    "        if new_time_left == 0\n",
    "            return u\n",
    "        else\n",
    "            eus = []\n",
    "            next_state = transition(state,action)\n",
    "            trace, = generate(act, (next_state,new_time_left), choicemap((:factor,1)))\n",
    "            push!(eus,expected_utility(next_state, trace[:action], new_time_left))\n",
    "            return u + mean(eus)\n",
    "        end\n",
    "    end\n",
    "end\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "make_agent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent actions: \n",
      "1\n",
      "WARNING: both Gen and Base export \"ifelse\"; uses of it in module Main must be qualified\n"
     ]
    }
   ],
   "source": [
    "act = make_agent().act\n",
    "start_state = 0\n",
    "total_time = 9\n",
    "amount_of_computation = 1000\n",
    "trace, = generate(act,(start_state,total_time), choicemap((:factor,1)))\n",
    "actions = []\n",
    "for i = 1:amount_of_computation\n",
    "    trace, = Gen.mh(trace, select(:action))\n",
    "    push!(actions, trace[:action])\n",
    "end\n",
    "print(\"Agent actions: \")\n",
    "println(trace[:action])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code computes the agent's initial action, given that the agent will get to take four actions in total. To simulate the agent's entire trajectory, we add a third function `simulate`, which updates and stores the world state in response to the agent's actions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "var transition = function(state, action) {\n",
    "  return state + action;\n",
    "};\n",
    "\n",
    "var utility = function(state) {\n",
    "  if (state === 3) {\n",
    "    return 1;\n",
    "  } else {\n",
    "    return 0;\n",
    "  }\n",
    "};\n",
    "\n",
    "var makeAgent = function() { \n",
    "  var act = function(state, timeLeft) {\n",
    "    return Infer({ model() {\n",
    "      var action = uniformDraw([-1, 0, 1]);\n",
    "      var eu = expectedUtility(state, action, timeLeft);\n",
    "      factor(100 * eu);\n",
    "      return action;\n",
    "    }});\n",
    "  };\n",
    "\n",
    "  var expectedUtility = function(state, action, timeLeft) {\n",
    "    var u = utility(state, action);\n",
    "    var newTimeLeft = timeLeft - 1;\n",
    "    if (newTimeLeft === 0) {\n",
    "      return u; \n",
    "    } else {\n",
    "      return u + expectation(Infer({ model() {\n",
    "        var nextState = transition(state, action); \n",
    "        var nextAction = sample(act(nextState, newTimeLeft));\n",
    "        return expectedUtility(nextState, nextAction, newTimeLeft);\n",
    "      }}));\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return { act };\n",
    "}\n",
    "\n",
    "\n",
    "var act = makeAgent().act;\n",
    "\n",
    "var simulate = function(state, timeLeft){\n",
    "  if (timeLeft === 0){\n",
    "    return [];\n",
    "  } else {\n",
    "    var action = sample(act(state, timeLeft));\n",
    "    var nextState = transition(state, action); \n",
    "    return [state].concat(simulate(nextState, timeLeft - 1))\n",
    "  }\n",
    "};\n",
    "\n",
    "var startState = 0;\n",
    "var totalTime = 4;\n",
    "print(\"Agent's trajectory: \" + simulate(startState, totalTime));\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "function transition(state,action)\n",
    "    return state+action\n",
    "end\n",
    "\n",
    "function utility(state)\n",
    "  if state == 3\n",
    "    return 1\n",
    "  else\n",
    "    return 0\n",
    "  end\n",
    "end\n",
    "\n",
    "function make_agent()\n",
    "    @gen function act(state,time_left)\n",
    "        action = @trace(uniform_discrete(-1,1),:action)\n",
    "        eu = expected_utility(state,action,time_left)\n",
    "        @trace(bernoulli(exp(100 * eu)),:factor)\n",
    "    end\n",
    "    function expected_utility(state,action,time_left)\n",
    "        u = utility(state)\n",
    "        new_time_left = time_left-1\n",
    "        if new_time_left == 0\n",
    "            return u\n",
    "        else\n",
    "            eus = []\n",
    "            next_state = transition(state,action)\n",
    "            trace, = generate(act, (next_state,new_time_left), choicemap((:factor,1)))\n",
    "            push!(eus,expected_utility(next_state, trace[:action], new_time_left))\n",
    "            return u + mean(eus)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function simulate(start_state,total_time,steps_to_simulate)\n",
    "    states = [start_state]\n",
    "    act = make_agent().act\n",
    "    amount_of_computation = 1000\n",
    "    for i = 1:steps_to_simulate\n",
    "        trace, = generate(act,(start_state,total_time), choicemap((:factor,1)))\n",
    "        for i = 1:amount_of_computation\n",
    "            trace, = Gen.mh(trace, select(:action))\n",
    "        end\n",
    "        next_state = transition(start_state,trace[:action])\n",
    "        start_state = next_state\n",
    "        total_time = total_time - 1\n",
    "        push!(states,next_state)\n",
    "    end\n",
    "    return states\n",
    "end\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "simulate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent's trajectory: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "start_state = 0\n",
    "total_time = 4;\n",
    "print(\"Agent's trajectory: \")\n",
    "println(simulate(start_state, total_time,total_time-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ">**Exercise**: Change the world such that it is a loop, i.e. moving right from state `3` moves to state `0`, and moving left from state `0` moves to state `3`. How does this change the agent's sequence of actions?\n",
    "\n",
    ">**Exercise**: Change the agent's action space such that the agent can also move two steps at a time. How does this change the agent's sequence of actions?\n",
    "\n",
    ">**Exercise**: Change the agent's utility function such that the agent moves as far as possible to the right, given its available total time.\n",
    "\n",
    "The `expectedUtility` and `simulate` functions are similar. The `expectedUtilty` function includes the agent's own (*subjective*) simulation of the future distribution on states. In the case of an MDP and optimal agent, the agent's simulation is identical to the world simulator. In later chapters, we describe agents whose subjective simulations differ from the world simulator. These agents either have inaccurate models of their own future choices or innacurate models of the world.\n",
    "\n",
    "We already mentioned the mutual recursion between `act` and `expectedUtility`. What does this recursion look like if we unroll it? In this example we get a tree that expands until `timeLeft` reaches zero. The root is the starting state (`startState === 0`) and this branches into three successor states (`-1`, `0`, `1`). This leads to an exponential blow-up in the runtime of a single action (which depends on how long into the future the agent plans):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: transition, utility, makeAgent, act, and simulate as above\n",
    "var transition = function(state, action) {\n",
    "  return state + action;\n",
    "};\n",
    "\n",
    "var utility = function(state) {\n",
    "  if (state === 3) {\n",
    "    return 1;\n",
    "  } else {\n",
    "    return 0;\n",
    "  }\n",
    "};\n",
    "\n",
    "var makeAgent = function() { \n",
    "\n",
    "  var act = function(state, timeLeft) {\n",
    "    return Infer({ model() {\n",
    "      var action = uniformDraw([-1, 0, 1]);\n",
    "      var eu = expectedUtility(state, action, timeLeft);\n",
    "      factor(100 * eu);\n",
    "      return action;\n",
    "    }});\n",
    "  };\n",
    "\n",
    "  var expectedUtility = function(state, action, timeLeft) {\n",
    "    var u = utility(state, action);\n",
    "    var newTimeLeft = timeLeft - 1;\n",
    "    if (newTimeLeft === 0) {\n",
    "      return u; \n",
    "    } else {\n",
    "      return u + expectation(Infer({ model() {\n",
    "        var nextState = transition(state, action); \n",
    "        var nextAction = sample(act(nextState, newTimeLeft));\n",
    "        return expectedUtility(nextState, nextAction, newTimeLeft);\n",
    "      }}));\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return { act };\n",
    "}\n",
    "\n",
    "\n",
    "var act = makeAgent().act;\n",
    "\n",
    "var simulate = function(state, timeLeft){\n",
    "  if (timeLeft === 0){\n",
    "    return [];\n",
    "  } else {\n",
    "    var action = sample(act(state, timeLeft));\n",
    "    var nextState = transition(state, action); \n",
    "    return [state].concat(simulate(nextState, timeLeft - 1))\n",
    "  }\n",
    "};\n",
    "///\n",
    "\n",
    "var startState = 0;\n",
    "\n",
    "var getRuntime = function(totalTime) {\n",
    "  return timeit(function() {\n",
    "    return act(startState, totalTime);\n",
    "  }).runtimeInMilliseconds.toPrecision(4);\n",
    "};\n",
    "\n",
    "var numSteps = [3, 4, 5, 6, 7];\n",
    "var runtimes = map(getRuntime, numSteps);\n",
    "\n",
    "print('Runtime in ms for for a given number of steps: \\n')\n",
    "print(_.zipObject(numSteps, runtimes));\n",
    "viz.bar(numSteps, runtimes);\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "function transition(state,action)\n",
    "    return state+action\n",
    "end\n",
    "\n",
    "function utility(state)\n",
    "  if state == 3\n",
    "    return 1\n",
    "  else\n",
    "    return 0\n",
    "  end\n",
    "end\n",
    "\n",
    "function make_agent()\n",
    "    @gen function act(state,time_left)\n",
    "        action = @trace(uniform_discrete(-1,1),:action)\n",
    "        eu = expected_utility(state,action,time_left)\n",
    "        @trace(bernoulli(exp(100 * eu)),:factor)\n",
    "    end\n",
    "    function expected_utility(state,action,time_left)\n",
    "        u = utility(state)\n",
    "        new_time_left = time_left-1\n",
    "        if new_time_left == 0\n",
    "            return u\n",
    "        else\n",
    "            eus = []\n",
    "            next_state = transition(state,action)\n",
    "            trace, = generate(act, (next_state,new_time_left), choicemap((:factor,1)))\n",
    "            push!(eus,expected_utility(next_state, trace[:action], new_time_left))\n",
    "            return u + mean(eus)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function simulate(start_state,total_time,steps_to_simulate)\n",
    "    states = [start_state]\n",
    "    act = make_agent().act\n",
    "    amount_of_computation = 1000\n",
    "    for i = 1:steps_to_simulate\n",
    "        trace, = generate(act,(start_state,total_time), choicemap((:factor,1)))\n",
    "        for i = 1:amount_of_computation\n",
    "            trace, = Gen.mh(trace, select(:action))\n",
    "        end\n",
    "        next_state = transition(start_state,trace[:action])\n",
    "        start_state = next_state\n",
    "        total_time = total_time - 1\n",
    "        push!(states,next_state)\n",
    "    end\n",
    "    return states\n",
    "end\n",
    "\n",
    "start_state = 0\n",
    "function get_run_time(total_time)\n",
    "    return @elapsed simulate(start_state,total_time,1)\n",
    "end\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "get_run_time (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Runtime in ms for for a given number of steps:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip890\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip890)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip891\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip890)\" d=\"\nM156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip892\">\n    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  412.189,1486.45 412.189,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  749.184,1486.45 749.184,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1086.18,1486.45 1086.18,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1423.17,1486.45 1423.17,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1760.17,1486.45 1760.17,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2097.17,1486.45 2097.17,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  412.189,1486.45 412.189,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  749.184,1486.45 749.184,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1086.18,1486.45 1086.18,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1423.17,1486.45 1423.17,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1760.17,1486.45 1760.17,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2097.17,1486.45 2097.17,1469.18 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M416.437 1528.49 Q419.793 1529.2 421.668 1531.47 Q423.566 1533.74 423.566 1537.07 Q423.566 1542.19 420.048 1544.99 Q416.529 1547.79 410.048 1547.79 Q407.872 1547.79 405.557 1547.35 Q403.265 1546.93 400.812 1546.08 L400.812 1541.56 Q402.756 1542.7 405.071 1543.28 Q407.386 1543.86 409.909 1543.86 Q414.307 1543.86 416.599 1542.12 Q418.913 1540.38 418.913 1537.07 Q418.913 1534.02 416.761 1532.31 Q414.631 1530.57 410.812 1530.57 L406.784 1530.57 L406.784 1526.73 L410.997 1526.73 Q414.446 1526.73 416.275 1525.36 Q418.103 1523.97 418.103 1521.38 Q418.103 1518.72 416.205 1517.31 Q414.33 1515.87 410.812 1515.87 Q408.89 1515.87 406.691 1516.29 Q404.492 1516.7 401.853 1517.58 L401.853 1513.42 Q404.515 1512.68 406.83 1512.31 Q409.168 1511.93 411.228 1511.93 Q416.552 1511.93 419.654 1514.37 Q422.756 1516.77 422.756 1520.89 Q422.756 1523.76 421.112 1525.75 Q419.469 1527.72 416.437 1528.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M752.193 1516.63 L740.388 1535.08 L752.193 1535.08 L752.193 1516.63 M750.967 1512.56 L756.846 1512.56 L756.846 1535.08 L761.777 1535.08 L761.777 1538.97 L756.846 1538.97 L756.846 1547.12 L752.193 1547.12 L752.193 1538.97 L736.592 1538.97 L736.592 1534.46 L750.967 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1076.46 1512.56 L1094.81 1512.56 L1094.81 1516.5 L1080.74 1516.5 L1080.74 1524.97 Q1081.76 1524.62 1082.78 1524.46 Q1083.8 1524.27 1084.81 1524.27 Q1090.6 1524.27 1093.98 1527.44 Q1097.36 1530.62 1097.36 1536.03 Q1097.36 1541.61 1093.89 1544.71 Q1090.42 1547.79 1084.1 1547.79 Q1081.92 1547.79 1079.65 1547.42 Q1077.41 1547.05 1075 1546.31 L1075 1541.61 Q1077.08 1542.74 1079.3 1543.3 Q1081.53 1543.86 1084 1543.86 Q1088.01 1543.86 1090.35 1541.75 Q1092.68 1539.64 1092.68 1536.03 Q1092.68 1532.42 1090.35 1530.31 Q1088.01 1528.21 1084 1528.21 Q1082.13 1528.21 1080.25 1528.62 Q1078.4 1529.04 1076.46 1529.92 L1076.46 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1423.58 1527.98 Q1420.43 1527.98 1418.58 1530.13 Q1416.75 1532.28 1416.75 1536.03 Q1416.75 1539.76 1418.58 1541.93 Q1420.43 1544.09 1423.58 1544.09 Q1426.73 1544.09 1428.56 1541.93 Q1430.41 1539.76 1430.41 1536.03 Q1430.41 1532.28 1428.56 1530.13 Q1426.73 1527.98 1423.58 1527.98 M1432.86 1513.32 L1432.86 1517.58 Q1431.1 1516.75 1429.3 1516.31 Q1427.51 1515.87 1425.76 1515.87 Q1421.13 1515.87 1418.67 1519 Q1416.24 1522.12 1415.89 1528.44 Q1417.26 1526.43 1419.32 1525.36 Q1421.38 1524.27 1423.86 1524.27 Q1429.07 1524.27 1432.08 1527.44 Q1435.11 1530.59 1435.11 1536.03 Q1435.11 1541.36 1431.96 1544.57 Q1428.81 1547.79 1423.58 1547.79 Q1417.58 1547.79 1414.41 1543.21 Q1411.24 1538.6 1411.24 1529.87 Q1411.24 1521.68 1415.13 1516.82 Q1419.02 1511.93 1425.57 1511.93 Q1427.33 1511.93 1429.11 1512.28 Q1430.92 1512.63 1432.86 1513.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1749.06 1512.56 L1771.28 1512.56 L1771.28 1514.55 L1758.73 1547.12 L1753.85 1547.12 L1765.66 1516.5 L1749.06 1516.5 L1749.06 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2097.17 1530.71 Q2093.83 1530.71 2091.91 1532.49 Q2090.01 1534.27 2090.01 1537.4 Q2090.01 1540.52 2091.91 1542.31 Q2093.83 1544.09 2097.17 1544.09 Q2100.5 1544.09 2102.42 1542.31 Q2104.34 1540.5 2104.34 1537.4 Q2104.34 1534.27 2102.42 1532.49 Q2100.52 1530.71 2097.17 1530.71 M2092.49 1528.72 Q2089.48 1527.98 2087.79 1525.92 Q2086.12 1523.86 2086.12 1520.89 Q2086.12 1516.75 2089.06 1514.34 Q2092.03 1511.93 2097.17 1511.93 Q2102.33 1511.93 2105.27 1514.34 Q2108.21 1516.75 2108.21 1520.89 Q2108.21 1523.86 2106.52 1525.92 Q2104.85 1527.98 2101.86 1528.72 Q2105.24 1529.5 2107.12 1531.8 Q2109.02 1534.09 2109.02 1537.4 Q2109.02 1542.42 2105.94 1545.11 Q2102.88 1547.79 2097.17 1547.79 Q2091.45 1547.79 2088.37 1545.11 Q2085.31 1542.42 2085.31 1537.4 Q2085.31 1534.09 2087.21 1531.8 Q2089.11 1529.5 2092.49 1528.72 M2090.78 1521.33 Q2090.78 1524.02 2092.44 1525.52 Q2094.13 1527.03 2097.17 1527.03 Q2100.17 1527.03 2101.86 1525.52 Q2103.58 1524.02 2103.58 1521.33 Q2103.58 1518.65 2101.86 1517.14 Q2100.17 1515.64 2097.17 1515.64 Q2094.13 1515.64 2092.44 1517.14 Q2090.78 1518.65 2090.78 1521.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,1445.72 2352.76,1445.72 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,1148.72 2352.76,1148.72 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,851.726 2352.76,851.726 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,554.731 2352.76,554.731 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,257.736 2352.76,257.736 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1486.45 156.598,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1445.72 182.952,1445.72 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1148.72 182.952,1148.72 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,851.726 182.952,851.726 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,554.731 182.952,554.731 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,257.736 182.952,257.736 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M63.4226 1431.51 Q59.8115 1431.51 57.9828 1435.08 Q56.1773 1438.62 56.1773 1445.75 Q56.1773 1452.86 57.9828 1456.42 Q59.8115 1459.96 63.4226 1459.96 Q67.0569 1459.96 68.8624 1456.42 Q70.6911 1452.86 70.6911 1445.75 Q70.6911 1438.62 68.8624 1435.08 Q67.0569 1431.51 63.4226 1431.51 M63.4226 1427.81 Q69.2328 1427.81 72.2883 1432.42 Q75.367 1437 75.367 1445.75 Q75.367 1454.48 72.2883 1459.08 Q69.2328 1463.67 63.4226 1463.67 Q57.6125 1463.67 54.5338 1459.08 Q51.4782 1454.48 51.4782 1445.75 Q51.4782 1437 54.5338 1432.42 Q57.6125 1427.81 63.4226 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M83.5845 1457.12 L88.4688 1457.12 L88.4688 1463 L83.5845 1463 L83.5845 1457.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M108.654 1431.51 Q105.043 1431.51 103.214 1435.08 Q101.409 1438.62 101.409 1445.75 Q101.409 1452.86 103.214 1456.42 Q105.043 1459.96 108.654 1459.96 Q112.288 1459.96 114.094 1456.42 Q115.922 1452.86 115.922 1445.75 Q115.922 1438.62 114.094 1435.08 Q112.288 1431.51 108.654 1431.51 M108.654 1427.81 Q114.464 1427.81 117.52 1432.42 Q120.598 1437 120.598 1445.75 Q120.598 1454.48 117.52 1459.08 Q114.464 1463.67 108.654 1463.67 Q102.844 1463.67 99.765 1459.08 Q96.7095 1454.48 96.7095 1445.75 Q96.7095 1437 99.765 1432.42 Q102.844 1427.81 108.654 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M64.6495 1134.52 Q61.0384 1134.52 59.2097 1138.08 Q57.4041 1141.63 57.4041 1148.76 Q57.4041 1155.86 59.2097 1159.43 Q61.0384 1162.97 64.6495 1162.97 Q68.2837 1162.97 70.0892 1159.43 Q71.9179 1155.86 71.9179 1148.76 Q71.9179 1141.63 70.0892 1138.08 Q68.2837 1134.52 64.6495 1134.52 M64.6495 1130.82 Q70.4596 1130.82 73.5152 1135.42 Q76.5938 1140.01 76.5938 1148.76 Q76.5938 1157.48 73.5152 1162.09 Q70.4596 1166.67 64.6495 1166.67 Q58.8393 1166.67 55.7606 1162.09 Q52.7051 1157.48 52.7051 1148.76 Q52.7051 1140.01 55.7606 1135.42 Q58.8393 1130.82 64.6495 1130.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M84.8114 1160.12 L89.6956 1160.12 L89.6956 1166 L84.8114 1166 L84.8114 1160.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M100.691 1162.07 L108.33 1162.07 L108.33 1135.7 L100.02 1137.37 L100.02 1133.11 L108.283 1131.44 L112.959 1131.44 L112.959 1162.07 L120.598 1162.07 L120.598 1166 L100.691 1166 L100.691 1162.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M65.0198 837.525 Q61.4087 837.525 59.58 841.09 Q57.7745 844.631 57.7745 851.761 Q57.7745 858.867 59.58 862.432 Q61.4087 865.974 65.0198 865.974 Q68.6541 865.974 70.4596 862.432 Q72.2883 858.867 72.2883 851.761 Q72.2883 844.631 70.4596 841.09 Q68.6541 837.525 65.0198 837.525 M65.0198 833.821 Q70.83 833.821 73.8855 838.428 Q76.9642 843.011 76.9642 851.761 Q76.9642 860.488 73.8855 865.094 Q70.83 869.677 65.0198 869.677 Q59.2097 869.677 56.131 865.094 Q53.0754 860.488 53.0754 851.761 Q53.0754 843.011 56.131 838.428 Q59.2097 833.821 65.0198 833.821 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M85.1818 863.126 L90.066 863.126 L90.066 869.006 L85.1818 869.006 L85.1818 863.126 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M104.279 865.071 L120.598 865.071 L120.598 869.006 L98.6539 869.006 L98.6539 865.071 Q101.316 862.316 105.899 857.687 Q110.506 853.034 111.686 851.691 Q113.932 849.168 114.811 847.432 Q115.714 845.673 115.714 843.983 Q115.714 841.228 113.77 839.492 Q111.848 837.756 108.746 837.756 Q106.547 837.756 104.094 838.52 Q101.663 839.284 98.8854 840.835 L98.8854 836.113 Q101.709 834.978 104.163 834.4 Q106.617 833.821 108.654 833.821 Q114.024 833.821 117.219 836.506 Q120.413 839.191 120.413 843.682 Q120.413 845.812 119.603 847.733 Q118.816 849.631 116.709 852.224 Q116.131 852.895 113.029 856.113 Q109.927 859.307 104.279 865.071 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M64.0708 540.53 Q60.4597 540.53 58.631 544.095 Q56.8254 547.636 56.8254 554.766 Q56.8254 561.872 58.631 565.437 Q60.4597 568.979 64.0708 568.979 Q67.705 568.979 69.5105 565.437 Q71.3392 561.872 71.3392 554.766 Q71.3392 547.636 69.5105 544.095 Q67.705 540.53 64.0708 540.53 M64.0708 536.826 Q69.8809 536.826 72.9365 541.433 Q76.0151 546.016 76.0151 554.766 Q76.0151 563.493 72.9365 568.099 Q69.8809 572.683 64.0708 572.683 Q58.2606 572.683 55.1819 568.099 Q52.1264 563.493 52.1264 554.766 Q52.1264 546.016 55.1819 541.433 Q58.2606 536.826 64.0708 536.826 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M84.2327 566.132 L89.1169 566.132 L89.1169 572.011 L84.2327 572.011 L84.2327 566.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M113.469 553.377 Q116.825 554.095 118.7 556.363 Q120.598 558.632 120.598 561.965 Q120.598 567.081 117.08 569.882 Q113.561 572.683 107.08 572.683 Q104.904 572.683 102.589 572.243 Q100.297 571.826 97.8437 570.97 L97.8437 566.456 Q99.7882 567.59 102.103 568.169 Q104.418 568.747 106.941 568.747 Q111.339 568.747 113.631 567.011 Q115.945 565.275 115.945 561.965 Q115.945 558.909 113.793 557.197 Q111.663 555.46 107.844 555.46 L103.816 555.46 L103.816 551.618 L108.029 551.618 Q111.478 551.618 113.307 550.252 Q115.135 548.863 115.135 546.271 Q115.135 543.609 113.237 542.197 Q111.362 540.761 107.844 540.761 Q105.922 540.761 103.723 541.178 Q101.524 541.595 98.8854 542.474 L98.8854 538.308 Q101.547 537.567 103.862 537.197 Q106.2 536.826 108.26 536.826 Q113.584 536.826 116.686 539.257 Q119.788 541.664 119.788 545.785 Q119.788 548.655 118.145 550.646 Q116.501 552.613 113.469 553.377 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M62.9365 243.535 Q59.3254 243.535 57.4967 247.1 Q55.6912 250.642 55.6912 257.771 Q55.6912 264.878 57.4967 268.442 Q59.3254 271.984 62.9365 271.984 Q66.5707 271.984 68.3763 268.442 Q70.205 264.878 70.205 257.771 Q70.205 250.642 68.3763 247.1 Q66.5707 243.535 62.9365 243.535 M62.9365 239.831 Q68.7467 239.831 71.8022 244.438 Q74.8809 249.021 74.8809 257.771 Q74.8809 266.498 71.8022 271.104 Q68.7467 275.688 62.9365 275.688 Q57.1264 275.688 54.0477 271.104 Q50.9921 266.498 50.9921 257.771 Q50.9921 249.021 54.0477 244.438 Q57.1264 239.831 62.9365 239.831 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M83.0984 269.137 L87.9827 269.137 L87.9827 275.016 L83.0984 275.016 L83.0984 269.137 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M111.015 244.53 L99.2095 262.979 L111.015 262.979 L111.015 244.53 M109.788 240.456 L115.668 240.456 L115.668 262.979 L120.598 262.979 L120.598 266.868 L115.668 266.868 L115.668 275.016 L111.015 275.016 L111.015 266.868 L95.4132 266.868 L95.4132 262.354 L109.788 240.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip892)\" d=\"\nM277.391 1399.25 L277.391 1445.72 L546.987 1445.72 L546.987 1399.25 L277.391 1399.25 L277.391 1399.25  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  277.391,1399.25 277.391,1445.72 546.987,1445.72 546.987,1399.25 277.391,1399.25 \n  \"/>\n<path clip-path=\"url(#clip892)\" d=\"\nM614.386 1377.4 L614.386 1445.72 L883.982 1445.72 L883.982 1377.4 L614.386 1377.4 L614.386 1377.4  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  614.386,1377.4 614.386,1445.72 883.982,1445.72 883.982,1377.4 614.386,1377.4 \n  \"/>\n<path clip-path=\"url(#clip892)\" d=\"\nM951.381 1258.67 L951.381 1445.72 L1220.98 1445.72 L1220.98 1258.67 L951.381 1258.67 L951.381 1258.67  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  951.381,1258.67 951.381,1445.72 1220.98,1445.72 1220.98,1258.67 951.381,1258.67 \n  \"/>\n<path clip-path=\"url(#clip892)\" d=\"\nM1288.38 1095.55 L1288.38 1445.72 L1557.97 1445.72 L1557.97 1095.55 L1288.38 1095.55 L1288.38 1095.55  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1288.38,1095.55 1288.38,1445.72 1557.97,1445.72 1557.97,1095.55 1288.38,1095.55 \n  \"/>\n<path clip-path=\"url(#clip892)\" d=\"\nM1625.37 765.367 L1625.37 1445.72 L1894.97 1445.72 L1894.97 765.367 L1625.37 765.367 L1625.37 765.367  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1625.37,765.367 1625.37,1445.72 1894.97,1445.72 1894.97,765.367 1625.37,765.367 \n  \"/>\n<path clip-path=\"url(#clip892)\" d=\"\nM1962.37 87.9763 L1962.37 1445.72 L2231.96 1445.72 L2231.96 87.9763 L1962.37 87.9763 L1962.37 87.9763  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1962.37,87.9763 1962.37,1445.72 2231.96,1445.72 2231.96,87.9763 1962.37,87.9763 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"\nM1815.7 198.898 L2279.55 198.898 L2279.55 95.2176 L1815.7 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1815.7,198.898 2279.55,198.898 2279.55,95.2176 1815.7,95.2176 1815.7,198.898 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"\nM1840.1 167.794 L1986.51 167.794 L1986.51 126.322 L1840.1 126.322 L1840.1 167.794  Z\n  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1840.1,167.794 1986.51,167.794 1986.51,126.322 1840.1,126.322 1840.1,167.794 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M2026.1 142.393 Q2025.38 141.977 2024.52 141.791 Q2023.69 141.583 2022.67 141.583 Q2019.06 141.583 2017.11 143.944 Q2015.19 146.282 2015.19 150.68 L2015.19 164.338 L2010.91 164.338 L2010.91 138.412 L2015.19 138.412 L2015.19 142.44 Q2016.54 140.078 2018.69 138.944 Q2020.84 137.787 2023.92 137.787 Q2024.36 137.787 2024.89 137.856 Q2025.42 137.903 2026.07 138.018 L2026.1 142.393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2030.12 154.106 L2030.12 138.412 L2034.38 138.412 L2034.38 153.944 Q2034.38 157.625 2035.82 159.476 Q2037.25 161.305 2040.12 161.305 Q2043.57 161.305 2045.56 159.106 Q2047.58 156.907 2047.58 153.111 L2047.58 138.412 L2051.84 138.412 L2051.84 164.338 L2047.58 164.338 L2047.58 160.356 Q2046.03 162.717 2043.97 163.875 Q2041.93 165.009 2039.22 165.009 Q2034.75 165.009 2032.44 162.231 Q2030.12 159.453 2030.12 154.106 M2040.84 137.787 L2040.84 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2082.16 148.689 L2082.16 164.338 L2077.9 164.338 L2077.9 148.828 Q2077.9 145.148 2076.47 143.319 Q2075.03 141.49 2072.16 141.49 Q2068.71 141.49 2066.72 143.69 Q2064.73 145.889 2064.73 149.685 L2064.73 164.338 L2060.45 164.338 L2060.45 138.412 L2064.73 138.412 L2064.73 142.44 Q2066.26 140.102 2068.32 138.944 Q2070.4 137.787 2073.11 137.787 Q2077.58 137.787 2079.87 140.565 Q2082.16 143.319 2082.16 148.689 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2109.94 131.051 L2109.94 138.412 L2118.71 138.412 L2118.71 141.722 L2109.94 141.722 L2109.94 155.796 Q2109.94 158.967 2110.79 159.87 Q2111.67 160.773 2114.34 160.773 L2118.71 160.773 L2118.71 164.338 L2114.34 164.338 Q2109.41 164.338 2107.53 162.509 Q2105.66 160.657 2105.66 155.796 L2105.66 141.722 L2102.53 141.722 L2102.53 138.412 L2105.66 138.412 L2105.66 131.051 L2109.94 131.051 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2124.31 138.412 L2128.57 138.412 L2128.57 164.338 L2124.31 164.338 L2124.31 138.412 M2124.31 128.319 L2128.57 128.319 L2128.57 133.713 L2124.31 133.713 L2124.31 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2157.67 143.389 Q2159.27 140.518 2161.49 139.153 Q2163.71 137.787 2166.72 137.787 Q2170.77 137.787 2172.97 140.634 Q2175.17 143.458 2175.17 148.689 L2175.17 164.338 L2170.89 164.338 L2170.89 148.828 Q2170.89 145.102 2169.57 143.296 Q2168.25 141.49 2165.54 141.49 Q2162.23 141.49 2160.31 143.69 Q2158.39 145.889 2158.39 149.685 L2158.39 164.338 L2154.1 164.338 L2154.1 148.828 Q2154.1 145.078 2152.78 143.296 Q2151.47 141.49 2148.71 141.49 Q2145.45 141.49 2143.53 143.713 Q2141.6 145.912 2141.6 149.685 L2141.6 164.338 L2137.32 164.338 L2137.32 138.412 L2141.6 138.412 L2141.6 142.44 Q2143.06 140.055 2145.1 138.921 Q2147.14 137.787 2149.94 137.787 Q2152.76 137.787 2154.73 139.222 Q2156.72 140.657 2157.67 143.389 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2205.84 150.31 L2205.84 152.393 L2186.26 152.393 Q2186.53 156.791 2188.9 159.106 Q2191.28 161.398 2195.52 161.398 Q2197.97 161.398 2200.26 160.796 Q2202.58 160.194 2204.84 158.99 L2204.84 163.018 Q2202.55 163.99 2200.15 164.5 Q2197.74 165.009 2195.26 165.009 Q2189.06 165.009 2185.42 161.398 Q2181.81 157.787 2181.81 151.629 Q2181.81 145.264 2185.24 141.537 Q2188.69 137.787 2194.52 137.787 Q2199.75 137.787 2202.78 141.166 Q2205.84 144.523 2205.84 150.31 M2201.58 149.06 Q2201.53 145.565 2199.61 143.481 Q2197.71 141.398 2194.57 141.398 Q2191 141.398 2188.85 143.412 Q2186.72 145.426 2186.4 149.083 L2201.58 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2229.36 139.176 L2229.36 143.203 Q2227.55 142.277 2225.61 141.815 Q2223.66 141.352 2221.58 141.352 Q2218.41 141.352 2216.81 142.324 Q2215.24 143.296 2215.24 145.24 Q2215.24 146.722 2216.37 147.578 Q2217.51 148.412 2220.93 149.176 L2222.39 149.5 Q2226.93 150.472 2228.83 152.254 Q2230.75 154.014 2230.75 157.185 Q2230.75 160.796 2227.88 162.902 Q2225.03 165.009 2220.03 165.009 Q2217.95 165.009 2215.68 164.592 Q2213.43 164.199 2210.93 163.388 L2210.93 158.99 Q2213.29 160.217 2215.59 160.842 Q2217.88 161.444 2220.12 161.444 Q2223.13 161.444 2224.75 160.426 Q2226.37 159.384 2226.37 157.509 Q2226.37 155.773 2225.19 154.847 Q2224.03 153.921 2220.08 153.064 L2218.59 152.717 Q2214.64 151.884 2212.88 150.171 Q2211.12 148.435 2211.12 145.426 Q2211.12 141.768 2213.71 139.778 Q2216.3 137.787 2221.07 137.787 Q2223.43 137.787 2225.52 138.134 Q2227.6 138.481 2229.36 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "num_steps = [3, 4, 5, 6, 7, 8];\n",
    "run_times = map(get_run_time, num_steps)\n",
    "println(\"Runtime in ms for for a given number of steps:\")\n",
    "StatsPlots.bar(num_steps,run_times, label = \"run times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most of this computation is unnecessary. If the agent starts at `state === 0`, there are three ways the agent could be at `state === 0` again after two steps: either the agent stays put twice or the agent goes one step away and then returns. The code above computes `agent(0, totalTime-2)` three times, while it only needs to be computed once. This problem can be resolved by *memoization*, which stores the results of a function call for re-use when the function is called again on the same input. This use of memoization results in a runtime that is polynomial in the number of states and the total time. <!-- We explore the efficiency of these algorithms in more detail in Section VI. --> In WebPPL, we use the higher-order function `dp.cache` to memoize the `act` and `expectedUtility` functions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: transition, utility and makeAgent functions as above, but...\n",
    "// ...with `act` and `expectedUtility` wrapped in `dp.cache`\n",
    "var transition = function(state, action) {\n",
    "  return state + action;\n",
    "};\n",
    "\n",
    "var utility = function(state) {\n",
    "  if (state === 3) {\n",
    "    return 1;\n",
    "  } else {\n",
    "    return 0;\n",
    "  }\n",
    "};\n",
    "\n",
    "var makeAgent = function() { \n",
    "  var act = dp.cache(function(state, timeLeft) {\n",
    "    return Infer({ model() {\n",
    "      var action = uniformDraw([-1, 0, 1]);\n",
    "      var eu = expectedUtility(state, action, timeLeft);\n",
    "      factor(100 * eu);\n",
    "      return action;\n",
    "    }});\n",
    "  });\n",
    "\n",
    "  var expectedUtility = dp.cache(function(state, action, timeLeft) {\n",
    "    var u = utility(state, action);\n",
    "    var newTimeLeft = timeLeft - 1;\n",
    "    if (newTimeLeft === 0) {\n",
    "      return u; \n",
    "    } else {\n",
    "      return u + expectation(Infer({ model() {\n",
    "        var nextState = transition(state, action); \n",
    "        var nextAction = sample(act(nextState, newTimeLeft));\n",
    "        return expectedUtility(nextState, nextAction, newTimeLeft);\n",
    "      }}));\n",
    "    }\n",
    "  });\n",
    "\n",
    "  return { act };\n",
    "}\n",
    "\n",
    "\n",
    "var act = makeAgent().act;\n",
    "\n",
    "var simulate = function(state, timeLeft){\n",
    "  if (timeLeft === 0){\n",
    "    return [];\n",
    "  } else {\n",
    "    var action = sample(act(state, timeLeft));\n",
    "    var nextState = transition(state, action); \n",
    "    return [state].concat(simulate(nextState, timeLeft - 1))\n",
    "  }\n",
    "};\n",
    "///\n",
    "\n",
    "var startState = 0;\n",
    "\n",
    "var getRuntime = function(totalTime) {\n",
    "  return timeit(function() {\n",
    "    return act(startState, totalTime);\n",
    "  }).runtimeInMilliseconds.toPrecision(4);\n",
    "};\n",
    "\n",
    "var numSteps = [3, 4, 5, 6, 7];\n",
    "var runtimes = map(getRuntime, numSteps);\n",
    "\n",
    "print('WITH MEMOIZATION \\n');\n",
    "print('Runtime in ms for for a given number of steps: \\n')\n",
    "print(_.zipObject(numSteps, runtimes));\n",
    "viz.bar(numSteps, runtimes)\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO: write above code in julia and gen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">**Exercise**: Could we also memoize `simulate`? Why or why not?\n",
    "\n",
    "<a id='restaurant_choice'></a>\n",
    "\n",
    "## Choosing restaurants in Gridworld\n",
    "\n",
    "The agent model above that includes memoization allows us to solve Bob's \"Restaurant Choice\" problem efficiently. \n",
    "\n",
    "We extend the agent model above by adding a `terminateAfterAction` to certain states to halt simulations when the agent reaches these states. For the Restaurant Choice problem, the restaurants are assumed to be terminal states. After computing the agent's trajectory, we use the [webppl-agents library](https://github.com/agentmodels/webppl-agents) to animate it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~~~javascript\n",
    "///fold: Restaurant constants, tableToUtilityFunction\n",
    "\n",
    "var ___ = ' '; \n",
    "var DN = { name : 'Donut N' };\n",
    "var DS = { name : 'Donut S' };\n",
    "var V = { name : 'Veg' };\n",
    "var N = { name : 'Noodle' };\n",
    "\n",
    "var tableToUtilityFunction = function(table, feature) {\n",
    "  return function(state, action) {\n",
    "    var stateFeatureName = feature(state).name;\n",
    "    return stateFeatureName ? table[stateFeatureName] : table.timeCost;\n",
    "  };\n",
    "};\n",
    "///\n",
    "\n",
    "// Construct world\n",
    "\n",
    "var grid = [\n",
    "  ['#', '#', '#', '#',  V , '#'],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', DN , ___, '#', ___],\n",
    "  ['#', '#', '#', ___, '#', ___],\n",
    "  ['#', '#', '#', ___, ___, ___],\n",
    "  ['#', '#', '#', ___, '#',  N ],\n",
    "  [___, ___, ___, ___, '#', '#'],\n",
    "  [DS , '#', '#', ___, '#', '#']\n",
    "];\n",
    "\n",
    "var mdp = makeGridWorldMDP({\n",
    "  grid,\n",
    "  start: [3, 1],\n",
    "  totalTime: 9\n",
    "});\n",
    "\n",
    "var world = mdp.world;\n",
    "var transition = world.transition;\n",
    "var stateToActions = world.stateToActions;\n",
    "\n",
    "\n",
    "// Construct utility function\n",
    "\n",
    "var utilityTable = {\n",
    "  'Donut S': 1, \n",
    "  'Donut N': 1, \n",
    "  'Veg': 3,\n",
    "  'Noodle': 2, \n",
    "  'timeCost': -0.1\n",
    "};\n",
    "\n",
    "var utility = tableToUtilityFunction(utilityTable, world.feature);\n",
    "\n",
    "\n",
    "// Construct agent\n",
    "\n",
    "var makeAgent = function() {\n",
    "  \n",
    "  var act = dp.cache(function(state) {\n",
    "    return Infer({ model() {\n",
    "      var action = uniformDraw(stateToActions(state));\n",
    "      var eu = expectedUtility(state, action);\n",
    "      factor(100 * eu);\n",
    "      return action;\n",
    "    }});\n",
    "  });\n",
    "\n",
    "  var expectedUtility = dp.cache(function(state, action){\n",
    "    var u = utility(state, action);\n",
    "    if (state.terminateAfterAction){\n",
    "      return u; \n",
    "    } else {\n",
    "      return u + expectation(Infer({ model() {\n",
    "        var nextState = transition(state, action);\n",
    "        var nextAction = sample(act(nextState));\n",
    "        return expectedUtility(nextState, nextAction);\n",
    "      }}));\n",
    "    }\n",
    "  });\n",
    "  \n",
    "  return { act };\n",
    "};\n",
    "\n",
    "var act = makeAgent().act;\n",
    "\n",
    "\n",
    "// Generate and draw a trajectory\n",
    "\n",
    "var simulate = function(state) {\n",
    "  var action = sample(act(state));\n",
    "  var nextState = transition(state, action);\n",
    "  var out = [state, action];\n",
    "  if (state.terminateAfterAction) {\n",
    "    return [out];\n",
    "  } else {\n",
    "    return [out].concat(simulate(nextState));\n",
    "  }\n",
    "};\n",
    "\n",
    "var trajectory = simulate(mdp.startState);\n",
    "\n",
    "viz.gridworld(world, { trajectory: map(first, trajectory) });\n",
    "~~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO: write above code in julia and gen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">**Exercise**: Change the utility table such that the agent goes to `Donut S`. What ways are there to accomplish this outcome?\n",
    "\n",
    "### Noisy agents, stochastic environments\n",
    "\n",
    "This section looked at two MDPs that were essentially deterministic. Part of the difficulty of solving MDPs is that actions, rewards and transitions can be stochastic. The [next chapter](/chapters/3b-mdp-gridworld.html) explores both noisy agents and stochastic gridworld environments.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "julia"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.1",
   "language": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}