{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "using Gen, Statistics, Memoize, Distributions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following example we can clearly see that the best move for the agent, given he can perform 4 steps, would be going right, as it can result in total of $9.7$ points, whereas going left will result in a maximum of $-0.1$ points. However, the agent still consistently chooses to go left because for the first state this would result in higher reward, thus the inference lowers the probabilty of drawing right from the posterior, therefore the option of going right is never drawn and so never explored."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "function transition(state,action)\n",
    "    return state+action\n",
    "end\n",
    "\n",
    "function utility(state)\n",
    "  if state == 3\n",
    "    return 10\n",
    "  elseif state == -1\n",
    "    return 0\n",
    "  else\n",
    "    return -0.1\n",
    "  end\n",
    "end\n",
    "\n",
    "function make_agent()\n",
    "  @gen function act(state,time_left)\n",
    "    action = @trace(uniform_discrete(-1,1),:action)\n",
    "    eu = expected_utility(state,action,time_left)\n",
    "    @trace(bernoulli(exp(100 * eu)),:factor)\n",
    "    return action\n",
    "  end\n",
    "\n",
    "  @memoize function run_act(state, time_left)\n",
    "      actions = []\n",
    "      trace, = generate(act, (state,time_left), choicemap((:factor,1)))\n",
    "      for i = 1:1000\n",
    "          trace, = Gen.mh(trace, select(:action))\n",
    "          push!(actions, get_retval(trace))\n",
    "      end\n",
    "      return actions\n",
    "  end\n",
    "\n",
    "  @gen function reward(state, action, time_left)\n",
    "      next_state = transition(state, action)\n",
    "      actions = run_act(state, time_left)\n",
    "      rand_choice = @trace(uniform_discrete(1, length(actions)), :rand_choice)\n",
    "      next_action = actions[rand_choice]\n",
    "      return expected_utility(next_state, next_action, time_left)\n",
    "  end\n",
    "\n",
    "  @memoize function run_reward(state, action, time_left)\n",
    "      rewards = []\n",
    "      trace, = generate(reward, (state, action, time_left))\n",
    "      for i = 1:1000\n",
    "          trace, = Gen.mh(trace, select(:rand_choice))\n",
    "          push!(rewards, get_retval(trace))\n",
    "      end\n",
    "      return rewards\n",
    "  end\n",
    "\n",
    "  @memoize function expected_utility(state,action,time_left)\n",
    "      u = utility(state)\n",
    "      new_time_left = time_left - 1\n",
    "      if new_time_left == 0\n",
    "          return u\n",
    "      else\n",
    "          return u + mean(run_reward(state, action, new_time_left))\n",
    "      end\n",
    "  end\n",
    "  return run_act\n",
    "end;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "run_act = make_agent()\n",
    "start_state = 0\n",
    "total_time = 4\n",
    "actions = run_act(start_state, total_time)\n",
    "rand_choice = rand(DiscreteUniform(1, length(actions)))\n",
    "next_action = actions[rand_choice]\n",
    "print(\"Agent actions: \")\n",
    "println(next_action)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent actions: -1\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "file_extension": ".jl",
   "name": "julia",
   "mimetype": "application/julia",
   "version": "1.6.1"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.1",
   "language": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}